{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: ë¬¸ì„œ ë¡œë”ì™€ ë²¡í„° ì €ì¥ì†Œ ì‹¤ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ LangChainì˜ ë¬¸ì„œ ë¡œë”, í…ìŠ¤íŠ¸ ë¶„í• ê¸°, ì„ë² ë”©, ë²¡í„° ì €ì¥ì†Œë¥¼ ì‹¤ìŠµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: 1\n",
      "ë¬¸ì„œ ë‚´ìš©:\n",
      "LangChainì€ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì œê³µí•˜ì—¬ ë³µì¡í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ë¬¸ì„œ ë¡œë”, í…ìŠ¤íŠ¸ ë¶„í• ê¸°, ì„ë² ë”©, ë²¡í„° ì €ì¥ì†Œ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "ë©”íƒ€ë°ì´í„°: {'source': 'sample.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# ìƒ˜í”Œ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±\n",
    "sample_text = \"\"\"LangChainì€ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì œê³µí•˜ì—¬ ë³µì¡í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ë¬¸ì„œ ë¡œë”, í…ìŠ¤íŠ¸ ë¶„í• ê¸°, ì„ë² ë”©, ë²¡í„° ì €ì¥ì†Œ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "with open(\"sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# TextLoader ì‚¬ìš©\n",
    "loader = TextLoader(\"sample.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}\")\n",
    "print(f\"ë¬¸ì„œ ë‚´ìš©:\\n{documents[0].page_content}\")\n",
    "print(f\"ë©”íƒ€ë°ì´í„°: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì›¹ í˜ì´ì§€ ë¡œë”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¡œë“œëœ ì›¹ ë¬¸ì„œ ìˆ˜: 1\n",
      "ì²« 500ì:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction | ğŸ¦œï¸ğŸ”— LangChain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1ğŸ’¬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) A...\n",
      "\n",
      "ë©”íƒ€ë°ì´í„°: {'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ğŸ¦œï¸ğŸ”— LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# ì›¹ í˜ì´ì§€ ë¡œë“œ\n",
    "web_loader = WebBaseLoader(\"https://python.langchain.com/docs/introduction/\")\n",
    "web_docs = web_loader.load()\n",
    "\n",
    "print(f\"ë¡œë“œëœ ì›¹ ë¬¸ì„œ ìˆ˜: {len(web_docs)}\")\n",
    "print(f\"ì²« 500ì:\\n{web_docs[0].page_content[:500]}...\")\n",
    "print(f\"\\në©”íƒ€ë°ì´í„°: {web_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PDF ë¡œë”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF í˜ì´ì§€ ìˆ˜: 1\n",
      "ì²« í˜ì´ì§€ ë‚´ìš© (100ì):\n",
      "Life in ancient Greece was centered around the polis, or city-state, which served as the heart of \n",
      "s...\n"
     ]
    }
   ],
   "source": [
    "# PDF íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ì‹¤í–‰\n",
    "try:\n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "    \n",
    "    # test.pdf íŒŒì¼ì´ ìˆë‹¤ë©´ ë¡œë“œ\n",
    "    if os.path.exists(\"test.pdf\"):\n",
    "        pdf_loader = PyPDFLoader(\"test.pdf\")\n",
    "        pdf_docs = pdf_loader.load()\n",
    "        print(f\"PDF í˜ì´ì§€ ìˆ˜: {len(pdf_docs)}\")\n",
    "        print(f\"ì²« í˜ì´ì§€ ë‚´ìš© (100ì):\\n{pdf_docs[0].page_content[:100]}...\")\n",
    "    else:\n",
    "        print(\"test.pdf íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "except ImportError:\n",
    "    print(\"PyPDF ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install pypdf'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì¬ê·€ì  í…ìŠ¤íŠ¸ ë¶„í• ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¶„í• ëœ ì²­í¬ ìˆ˜: 5\n",
      "\n",
      "ì²­í¬ 1 (54ì):\n",
      "LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "ì²­í¬ 2 (91ì):\n",
      "ì£¼ìš” ê¸°ëŠ¥:\n",
      "1. í”„ë¡¬í”„íŠ¸ ê´€ë¦¬: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‰½ê²Œ ë§Œë“¤ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "2. ì²´ì¸: ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "ì²­í¬ 3 (69ì):\n",
      "3. ì—ì´ì „íŠ¸: LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
      "4. ë©”ëª¨ë¦¬: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "ì²­í¬ 4 (85ì):\n",
      "LangChainì˜ ì¥ì :\n",
      "- ëª¨ë“ˆì„±: ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ì¡°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- í™•ì¥ì„±: ì»¤ìŠ¤í…€ ì»´í¬ë„ŒíŠ¸ë¥¼ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "ì²­í¬ 5 (29ì):\n",
      "- í†µí•©: ë‹¤ì–‘í•œ LLM ì œê³µìì™€ ë„êµ¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ê¸´ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "long_text = \"\"\"\n",
    "LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì£¼ìš” ê¸°ëŠ¥:\n",
    "1. í”„ë¡¬í”„íŠ¸ ê´€ë¦¬: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‰½ê²Œ ë§Œë“¤ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "2. ì²´ì¸: ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "3. ì—ì´ì „íŠ¸: LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
    "4. ë©”ëª¨ë¦¬: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "LangChainì˜ ì¥ì :\n",
    "- ëª¨ë“ˆì„±: ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ì¡°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- í™•ì¥ì„±: ì»¤ìŠ¤í…€ ì»´í¬ë„ŒíŠ¸ë¥¼ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- í†µí•©: ë‹¤ì–‘í•œ LLM ì œê³µìì™€ ë„êµ¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ìƒì„±\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "chunks = text_splitter.split_text(long_text)\n",
    "\n",
    "print(f\"ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"ì²­í¬ {i} ({len(chunk)}ì):\\n{chunk}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì½”ë“œ ë¶„í• ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì½”ë“œ ì²­í¬ ìˆ˜: 5\n",
      "\n",
      "ì½”ë“œ ì²­í¬ 1:\n",
      "def hello_world():\n",
      "    \"\"\"ê°„ë‹¨í•œ í—¬ë¡œ ì›”ë“œ í•¨ìˆ˜\"\"\"\n",
      "    print(\"Hello, World!\")\n",
      "    return True\n",
      "--------------------------------------------------\n",
      "ì½”ë“œ ì²­í¬ 2:\n",
      "class Greeting:\n",
      "    \"\"\"ì¸ì‚¬ë§ í´ë˜ìŠ¤\"\"\"\n",
      "    def __init__(self, name):\n",
      "        self.name = name\n",
      "--------------------------------------------------\n",
      "ì½”ë“œ ì²­í¬ 3:\n",
      "def say_hello(self):\n",
      "        \"\"\"ì¸ì‚¬í•˜ê¸°\"\"\"\n",
      "        return f\"Hello, {self.name}!\"\n",
      "--------------------------------------------------\n",
      "ì½”ë“œ ì²­í¬ 4:\n",
      "def say_goodbye(self):\n",
      "        \"\"\"ì‘ë³„ ì¸ì‚¬\"\"\"\n",
      "        return f\"Goodbye, {self.name}!\"\n",
      "--------------------------------------------------\n",
      "ì½”ë“œ ì²­í¬ 5:\n",
      "if __name__ == \"__main__\":\n",
      "    hello_world()\n",
      "    greeter = Greeting(\"LangChain\")\n",
      "    print(greeter.say_hello())\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# íŒŒì´ì¬ ì½”ë“œ ì˜ˆì œ\n",
    "python_code = '''\n",
    "def hello_world():\n",
    "    \"\"\"ê°„ë‹¨í•œ í—¬ë¡œ ì›”ë“œ í•¨ìˆ˜\"\"\"\n",
    "    print(\"Hello, World!\")\n",
    "    return True\n",
    "\n",
    "class Greeting:\n",
    "    \"\"\"ì¸ì‚¬ë§ í´ë˜ìŠ¤\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def say_hello(self):\n",
    "        \"\"\"ì¸ì‚¬í•˜ê¸°\"\"\"\n",
    "        return f\"Hello, {self.name}!\"\n",
    "    \n",
    "    def say_goodbye(self):\n",
    "        \"\"\"ì‘ë³„ ì¸ì‚¬\"\"\"\n",
    "        return f\"Goodbye, {self.name}!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hello_world()\n",
    "    greeter = Greeting(\"LangChain\")\n",
    "    print(greeter.say_hello())\n",
    "'''\n",
    "\n",
    "# íŒŒì´ì¬ ì½”ë“œ ë¶„í• ê¸°\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "code_chunks = python_splitter.split_text(python_code)\n",
    "\n",
    "print(f\"ì½”ë“œ ì²­í¬ ìˆ˜: {len(code_chunks)}\\n\")\n",
    "for i, chunk in enumerate(code_chunks, 1):\n",
    "    print(f\"ì½”ë“œ ì²­í¬ {i}:\\n{chunk}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë§ˆí¬ë‹¤ìš´ ë¶„í• ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë§ˆí¬ë‹¤ìš´ ì²­í¬ ìˆ˜: 4\n",
      "\n",
      "MD ì²­í¬ 1:\n",
      "# LangChain ê°€ì´ë“œ\n",
      "\n",
      "## ì†Œê°œ\n",
      "LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "\n",
      "## ì£¼ìš” ê¸°ëŠ¥\n",
      "--------------------------------------------------\n",
      "MD ì²­í¬ 2:\n",
      "## ì£¼ìš” ê¸°ëŠ¥\n",
      "\n",
      "### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
      "ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "\n",
      "### ì²´ì¸\n",
      "ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "MD ì²­í¬ 3:\n",
      "## ì‹œì‘í•˜ê¸°\n",
      "```python\n",
      "from langchain import OpenAI\n",
      "llm = OpenAI()\n",
      "--------------------------------------------------\n",
      "MD ì²­í¬ 4:\n",
      "```\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "# LangChain ê°€ì´ë“œ\n",
    "\n",
    "## ì†Œê°œ\n",
    "LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ê¸°ëŠ¥\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì²´ì¸\n",
    "ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì‹œì‘í•˜ê¸°\n",
    "```python\n",
    "from langchain import OpenAI\n",
    "llm = OpenAI()\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# ë§ˆí¬ë‹¤ìš´ ë¶„í• ê¸°\n",
    "md_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "md_chunks = md_splitter.split_text(markdown_text)\n",
    "\n",
    "print(f\"ë§ˆí¬ë‹¤ìš´ ì²­í¬ ìˆ˜: {len(md_chunks)}\\n\")\n",
    "for i, chunk in enumerate(md_chunks, 1):\n",
    "    print(f\"MD ì²­í¬ {i}:\\n{chunk}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì„ë² ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ì°¨ì›: 1536\n",
      "ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© (ì²˜ìŒ 10ê°œ ê°’): [-0.006174771580845118, -0.024400196969509125, -0.019658733159303665, -0.03282342851161957, 0.018368078395724297, 0.00598796596750617, -0.01618075557053089, 0.02467191405594349, 0.008402852341532707, 0.010433937422931194]\n",
      "\n",
      "í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„:\n",
      "í…ìŠ¤íŠ¸ 1 <-> í…ìŠ¤íŠ¸ 2: 0.8110\n",
      "í…ìŠ¤íŠ¸ 1 <-> í…ìŠ¤íŠ¸ 3: 0.9208\n",
      "í…ìŠ¤íŠ¸ 2 <-> í…ìŠ¤íŠ¸ 3: 0.7723\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ìƒì„±\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì„ë² ë”©\n",
    "texts = [\n",
    "    \"LangChainì€ AI ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\",\n",
    "    \"íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    \"LangChainì„ ì‚¬ìš©í•˜ë©´ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# ì„ë² ë”© ìƒì„±\n",
    "embedded_texts = embeddings.embed_documents(texts)\n",
    "\n",
    "print(f\"ì„ë² ë”© ì°¨ì›: {len(embedded_texts[0])}\")\n",
    "print(f\"ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© (ì²˜ìŒ 10ê°œ ê°’): {embedded_texts[0][:10]}\")\n",
    "\n",
    "# ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "print(\"\\ní…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„:\")\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i+1, len(texts)):\n",
    "        sim = cosine_similarity(embedded_texts[i], embedded_texts[j])\n",
    "        print(f\"í…ìŠ¤íŠ¸ {i+1} <-> í…ìŠ¤íŠ¸ {j+1}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ë¬¸ì„œ ë¡œë“œ, ë¶„í• , ì„ë² ë”© í†µí•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ë¬¸ì„œ ê¸¸ì´: 226ì\n",
      "ë¶„í• ëœ ì²­í¬ ìˆ˜: 4\n",
      "ê° ì²­í¬ì˜ ì„ë² ë”© ì°¨ì›: 1536\n",
      "\n",
      "ì²­í¬ 1: ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥, ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥ì„ ì¸ê³µì ìœ¼ë¡œ êµ¬í˜„í•œ ì»´í“¨í„° ê³¼í•™ì˜ ...\n",
      "ì²­í¬ 2: ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ë¡œ, ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ì•Œê³ ë¦¬ì¦˜...\n",
      "ì²­í¬ 3: ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë°©ë²•ìœ¼ë¡œ, ì¸ê³µì‹ ê²½ë§ì„ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ìŒ“ì•„ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤....\n",
      "ì²­í¬ 4: ìì—°ì–´ì²˜ë¦¬(NLP)ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” AI ê¸°ìˆ ì…ë‹ˆ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±\n",
    "sample_content = \"\"\"\n",
    "ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥, ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥ì„ ì¸ê³µì ìœ¼ë¡œ êµ¬í˜„í•œ ì»´í“¨í„° ê³¼í•™ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ë¡œ, ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì—°êµ¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë°©ë²•ìœ¼ë¡œ, ì¸ê³µì‹ ê²½ë§ì„ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ìŒ“ì•„ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "ìì—°ì–´ì²˜ë¦¬(NLP)ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” AI ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"ai_intro.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_content)\n",
    "\n",
    "# 1. ë¬¸ì„œ ë¡œë“œ\n",
    "loader = TextLoader(\"ai_intro.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. ë¬¸ì„œ ë¶„í• \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. ì„ë² ë”© ìƒì„±\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_docs = embeddings.embed_documents([doc.page_content for doc in splits])\n",
    "\n",
    "print(f\"ì›ë³¸ ë¬¸ì„œ ê¸¸ì´: {len(documents[0].page_content)}ì\")\n",
    "print(f\"ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(splits)}\")\n",
    "print(f\"ê° ì²­í¬ì˜ ì„ë² ë”© ì°¨ì›: {len(embedded_docs[0])}\\n\")\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"ì²­í¬ {i+1}: {split.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. FAISS ë²¡í„° ì €ì¥ì†Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¿¼ë¦¬: 'íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°'\n",
      "\n",
      "ìœ ì‚¬í•œ ë¬¸ì„œ (ìƒìœ„ 3ê°œ):\n",
      "1. íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\n",
      "2. íŒŒì´ì¬ìœ¼ë¡œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "3. LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰:\n",
      "ì ìˆ˜: 0.1417 - íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\n",
      "ì ìˆ˜: 0.1853 - íŒŒì´ì¬ìœ¼ë¡œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ì ìˆ˜: 0.4124 - LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ìƒ˜í”Œ ë¬¸ì„œë“¤\n",
    "texts = [\n",
    "    \"íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    \"LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\",\n",
    "    \"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì„ë² ë”©ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.\",\n",
    "    \"íŒŒì´ì¬ìœ¼ë¡œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"FAISSëŠ” Facebookì´ ê°œë°œí•œ ë²¡í„° ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "# ìœ ì‚¬ë„ ê²€ìƒ‰\n",
    "query = \"íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: '{query}'\\n\")\n",
    "print(\"ìœ ì‚¬í•œ ë¬¸ì„œ (ìƒìœ„ 3ê°œ):\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "\n",
    "# ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\n",
    "print(\"\\nì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰:\")\n",
    "results_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"ì ìˆ˜: {score:.4f} - {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ë° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²¡í„° ì €ì¥ì†Œê°€ 'faiss_index' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì¿¼ë¦¬: 'ë²¡í„° ê²€ìƒ‰'\n",
      "ê²€ìƒ‰ ê²°ê³¼:\n",
      "1. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì„ë² ë”©ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
      "2. FAISSëŠ” Facebookì´ ê°œë°œí•œ ë²¡í„° ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ë²¡í„° ì €ì¥ì†Œ ì €ì¥\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(\"ë²¡í„° ì €ì¥ì†Œê°€ 'faiss_index' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    \"faiss_index\", \n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "print(\"ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë¡œë“œëœ ì €ì¥ì†Œë¡œ ê²€ìƒ‰\n",
    "query = \"ë²¡í„° ê²€ìƒ‰\"\n",
    "results = loaded_vectorstore.similarity_search(query, k=2)\n",
    "print(f\"\\nì¿¼ë¦¬: '{query}'\")\n",
    "print(\"ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ë©”íƒ€ë°ì´í„° í•„í„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¿¼ë¦¬: 'í”„ë¡œê·¸ë˜ë° ì–¸ì–´'\n",
      "í•„í„°: category='programming'\n",
      "\n",
      "í•„í„°ë§ëœ ê²°ê³¼:\n",
      "- íŒŒì´ì¬ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "  ë©”íƒ€ë°ì´í„°: {'category': 'programming', 'language': 'python'}\n",
      "- ìë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ì›¹ ê°œë°œì— í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n",
      "  ë©”íƒ€ë°ì´í„°: {'category': 'programming', 'language': 'javascript'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ë¬¸ì„œë“¤\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"íŒŒì´ì¬ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\",\n",
    "        metadata={\"category\": \"programming\", \"language\": \"python\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"ìë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ì›¹ ê°œë°œì— í•„ìˆ˜ì ì…ë‹ˆë‹¤.\",\n",
    "        metadata={\"category\": \"programming\", \"language\": \"javascript\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.\",\n",
    "        metadata={\"category\": \"ai\", \"topic\": \"ml\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\",\n",
    "        metadata={\"category\": \"ai\", \"topic\": \"dl\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "vectorstore_with_metadata = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê²€ìƒ‰\n",
    "query = \"í”„ë¡œê·¸ë˜ë° ì–¸ì–´\"\n",
    "filter_dict = {\"category\": \"programming\"}\n",
    "\n",
    "# í•„í„° ì ìš© ê²€ìƒ‰ (FAISSëŠ” ì§ì ‘ì ì¸ í•„í„°ë§ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì „ì²´ ê²€ìƒ‰ í›„ í•„í„°ë§)\n",
    "all_results = vectorstore_with_metadata.similarity_search(query, k=10)\n",
    "filtered_results = [\n",
    "    doc for doc in all_results \n",
    "    if doc.metadata.get(\"category\") == \"programming\"\n",
    "]\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: '{query}'\")\n",
    "print(f\"í•„í„°: category='programming'\\n\")\n",
    "print(\"í•„í„°ë§ëœ ê²°ê³¼:\")\n",
    "for doc in filtered_results:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "    print(f\"  ë©”íƒ€ë°ì´í„°: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "ë‹¤ìŒ ê³¼ì œë“¤ì„ ì‹œë„í•´ë³´ì„¸ìš”:\n",
    "\n",
    "1. ì—¬ëŸ¬ íŒŒì¼ì„ ë¡œë“œí•˜ê³  í†µí•©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œ ë§Œë“¤ê¸°\n",
    "2. ë‹¤ì–‘í•œ ì²­í¬ í¬ê¸°ë¡œ ì‹¤í—˜í•˜ê³  ê²€ìƒ‰ í’ˆì§ˆ ë¹„êµ\n",
    "3. ì»¤ìŠ¤í…€ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ê³ ê¸‰ í•„í„°ë§ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì‹¤ìŠµ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
