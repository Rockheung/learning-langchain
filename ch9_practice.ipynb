{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: LangGraph Deployment 실습\n",
    "\n",
    "이 노트북은 LangGraph 애플리케이션의 배포와 프로덕션 환경 구성을 실습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"OpenAI API Key를 입력하세요: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LangGraph Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any, TypedDict, Optional\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangGraph 설정 클래스\n",
    "@dataclass\n",
    "class LangGraphConfig:\n",
    "    \"\"\"LangGraph 애플리케이션 설정\"\"\"\n",
    "    app_name: str\n",
    "    version: str\n",
    "    environment: str  # development, staging, production\n",
    "    api_keys: Dict[str, str]\n",
    "    database_url: Optional[str] = None\n",
    "    redis_url: Optional[str] = None\n",
    "    max_workers: int = 4\n",
    "    timeout: int = 30\n",
    "    retry_policy: Dict[str, Any] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"app_name\": self.app_name,\n",
    "            \"version\": self.version,\n",
    "            \"environment\": self.environment,\n",
    "            \"max_workers\": self.max_workers,\n",
    "            \"timeout\": self.timeout,\n",
    "            \"retry_policy\": self.retry_policy or {\n",
    "                \"max_retries\": 3,\n",
    "                \"backoff_factor\": 2,\n",
    "                \"max_backoff\": 60\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def save_to_file(self, filepath: str):\n",
    "        \"\"\"설정을 파일로 저장\"\"\"\n",
    "        config_dict = self.to_dict()\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "        print(f\"설정이 {filepath}에 저장되었습니다.\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_file(cls, filepath: str):\n",
    "        \"\"\"파일에서 설정 로드\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        return cls(**config_dict)\n",
    "\n",
    "# 환경별 설정 생성\n",
    "def create_environment_config(env: str) -> LangGraphConfig:\n",
    "    \"\"\"환경별 설정 생성\"\"\"\n",
    "    base_config = {\n",
    "        \"app_name\": \"langgraph-app\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"api_keys\": {\n",
    "            \"openai\": os.getenv(\"OPENAI_API_KEY\", \"\"),\n",
    "            \"anthropic\": os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if env == \"development\":\n",
    "        return LangGraphConfig(\n",
    "            **base_config,\n",
    "            environment=\"development\",\n",
    "            database_url=\"sqlite:///dev.db\",\n",
    "            max_workers=2,\n",
    "            timeout=60\n",
    "        )\n",
    "    elif env == \"staging\":\n",
    "        return LangGraphConfig(\n",
    "            **base_config,\n",
    "            environment=\"staging\",\n",
    "            database_url=os.getenv(\"DATABASE_URL\"),\n",
    "            redis_url=os.getenv(\"REDIS_URL\"),\n",
    "            max_workers=4,\n",
    "            timeout=30\n",
    "        )\n",
    "    elif env == \"production\":\n",
    "        return LangGraphConfig(\n",
    "            **base_config,\n",
    "            environment=\"production\",\n",
    "            database_url=os.getenv(\"DATABASE_URL\"),\n",
    "            redis_url=os.getenv(\"REDIS_URL\"),\n",
    "            max_workers=8,\n",
    "            timeout=30,\n",
    "            retry_policy={\n",
    "                \"max_retries\": 5,\n",
    "                \"backoff_factor\": 2,\n",
    "                \"max_backoff\": 120\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    return LangGraphConfig(**base_config, environment=\"development\")\n",
    "\n",
    "# 테스트\n",
    "dev_config = create_environment_config(\"development\")\n",
    "prod_config = create_environment_config(\"production\")\n",
    "\n",
    "print(\"개발 환경 설정:\")\n",
    "print(json.dumps(dev_config.to_dict(), indent=2))\n",
    "\n",
    "print(\"\\n프로덕션 환경 설정:\")\n",
    "print(json.dumps(prod_config.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Deployment Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 배포 가능한 그래프 서비스\n",
    "class GraphDeploymentService:\n",
    "    \"\"\"LangGraph 배포 서비스\"\"\"\n",
    "    \n",
    "    def __init__(self, config: LangGraphConfig):\n",
    "        self.config = config\n",
    "        self.graphs = {}\n",
    "        self.metrics = {\n",
    "            \"requests\": 0,\n",
    "            \"errors\": 0,\n",
    "            \"avg_latency\": 0,\n",
    "            \"start_time\": datetime.now()\n",
    "        }\n",
    "    \n",
    "    def register_graph(self, name: str, graph_builder: Callable):\n",
    "        \"\"\"그래프 등록\"\"\"\n",
    "        try:\n",
    "            graph = graph_builder()\n",
    "            self.graphs[name] = graph\n",
    "            print(f\"✅ 그래프 '{name}' 등록 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 그래프 '{name}' 등록 실패: {e}\")\n",
    "    \n",
    "    async def invoke_graph(self, graph_name: str, input_data: Dict) -> Dict:\n",
    "        \"\"\"그래프 실행\"\"\"\n",
    "        if graph_name not in self.graphs:\n",
    "            raise ValueError(f\"그래프 '{graph_name}'을 찾을 수 없습니다.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.metrics[\"requests\"] += 1\n",
    "        \n",
    "        try:\n",
    "            # 타임아웃 적용\n",
    "            result = await asyncio.wait_for(\n",
    "                self._execute_graph(graph_name, input_data),\n",
    "                timeout=self.config.timeout\n",
    "            )\n",
    "            \n",
    "            # 메트릭 업데이트\n",
    "            latency = time.time() - start_time\n",
    "            self._update_metrics(latency)\n",
    "            \n",
    "            return result\n",
    "        except asyncio.TimeoutError:\n",
    "            self.metrics[\"errors\"] += 1\n",
    "            raise TimeoutError(f\"그래프 실행이 {self.config.timeout}초를 초과했습니다.\")\n",
    "        except Exception as e:\n",
    "            self.metrics[\"errors\"] += 1\n",
    "            raise e\n",
    "    \n",
    "    async def _execute_graph(self, graph_name: str, input_data: Dict) -> Dict:\n",
    "        \"\"\"실제 그래프 실행\"\"\"\n",
    "        graph = self.graphs[graph_name]\n",
    "        # 동기 함수를 비동기로 실행\n",
    "        loop = asyncio.get_event_loop()\n",
    "        result = await loop.run_in_executor(None, graph.invoke, input_data)\n",
    "        return result\n",
    "    \n",
    "    def _update_metrics(self, latency: float):\n",
    "        \"\"\"메트릭 업데이트\"\"\"\n",
    "        current_avg = self.metrics[\"avg_latency\"]\n",
    "        total_requests = self.metrics[\"requests\"]\n",
    "        self.metrics[\"avg_latency\"] = (\n",
    "            (current_avg * (total_requests - 1) + latency) / total_requests\n",
    "        )\n",
    "    \n",
    "    def get_health_status(self) -> Dict:\n",
    "        \"\"\"헬스 체크\"\"\"\n",
    "        uptime = (datetime.now() - self.metrics[\"start_time\"]).total_seconds()\n",
    "        error_rate = (\n",
    "            self.metrics[\"errors\"] / self.metrics[\"requests\"]\n",
    "            if self.metrics[\"requests\"] > 0 else 0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"healthy\" if error_rate < 0.1 else \"degraded\",\n",
    "            \"uptime_seconds\": uptime,\n",
    "            \"total_requests\": self.metrics[\"requests\"],\n",
    "            \"error_rate\": error_rate,\n",
    "            \"avg_latency_ms\": self.metrics[\"avg_latency\"] * 1000,\n",
    "            \"registered_graphs\": list(self.graphs.keys())\n",
    "        }\n",
    "\n",
    "# 샘플 그래프 빌더\n",
    "def build_chat_graph():\n",
    "    \"\"\"채팅 그래프 생성\"\"\"\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    \n",
    "    def chat_node(state: MessagesState):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    workflow.add_node(\"chat\", chat_node)\n",
    "    workflow.set_entry_point(\"chat\")\n",
    "    workflow.add_edge(\"chat\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def build_analysis_graph():\n",
    "    \"\"\"분석 그래프 생성\"\"\"\n",
    "    class AnalysisState(TypedDict):\n",
    "        text: str\n",
    "        analysis: str\n",
    "    \n",
    "    workflow = StateGraph(AnalysisState)\n",
    "    \n",
    "    def analyze_node(state: AnalysisState):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        prompt = f\"다음 텍스트를 분석하세요: {state['text']}\"\n",
    "        response = llm.invoke(prompt)\n",
    "        return {\"analysis\": response.content}\n",
    "    \n",
    "    workflow.add_node(\"analyze\", analyze_node)\n",
    "    workflow.set_entry_point(\"analyze\")\n",
    "    workflow.add_edge(\"analyze\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# 서비스 배포 시뮬레이션\n",
    "async def deploy_and_test():\n",
    "    \"\"\"배포 및 테스트\"\"\"\n",
    "    # 설정 생성\n",
    "    config = create_environment_config(\"staging\")\n",
    "    \n",
    "    # 서비스 초기화\n",
    "    service = GraphDeploymentService(config)\n",
    "    \n",
    "    # 그래프 등록\n",
    "    service.register_graph(\"chat\", build_chat_graph)\n",
    "    service.register_graph(\"analysis\", build_analysis_graph)\n",
    "    \n",
    "    print(\"\\n📊 서비스 상태:\")\n",
    "    print(json.dumps(service.get_health_status(), indent=2))\n",
    "    \n",
    "    # 테스트 요청\n",
    "    print(\"\\n🚀 테스트 요청 실행:\")\n",
    "    \n",
    "    # 채팅 그래프 테스트\n",
    "    chat_result = await service.invoke_graph(\n",
    "        \"chat\",\n",
    "        {\"messages\": [HumanMessage(content=\"안녕하세요!\")]}\n",
    "    )\n",
    "    print(f\"채팅 응답: {chat_result['messages'][-1].content[:50]}...\")\n",
    "    \n",
    "    # 분석 그래프 테스트\n",
    "    analysis_result = await service.invoke_graph(\n",
    "        \"analysis\",\n",
    "        {\"text\": \"LangGraph는 강력한 프레임워크입니다.\"}\n",
    "    )\n",
    "    print(f\"분석 결과: {analysis_result['analysis'][:50]}...\")\n",
    "    \n",
    "    # 최종 메트릭\n",
    "    print(\"\\n📈 최종 메트릭:\")\n",
    "    print(json.dumps(service.get_health_status(), indent=2))\n",
    "\n",
    "# 실행\n",
    "await deploy_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. API Server Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import uuid\n",
    "from queue import Queue\n",
    "from typing import Any\n",
    "\n",
    "# API 서버 구현\n",
    "class LangGraphAPIServer:\n",
    "    \"\"\"LangGraph API 서버\"\"\"\n",
    "    \n",
    "    def __init__(self, service: GraphDeploymentService, port: int = 5000):\n",
    "        self.service = service\n",
    "        self.port = port\n",
    "        self.app = Flask(__name__)\n",
    "        CORS(self.app)\n",
    "        self.request_queue = Queue()\n",
    "        self.response_cache = {}\n",
    "        self._setup_routes()\n",
    "    \n",
    "    def _setup_routes(self):\n",
    "        \"\"\"API 라우트 설정\"\"\"\n",
    "        \n",
    "        @self.app.route('/health', methods=['GET'])\n",
    "        def health_check():\n",
    "            return jsonify(self.service.get_health_status())\n",
    "        \n",
    "        @self.app.route('/graphs', methods=['GET'])\n",
    "        def list_graphs():\n",
    "            return jsonify({\n",
    "                \"graphs\": list(self.service.graphs.keys()),\n",
    "                \"count\": len(self.service.graphs)\n",
    "            })\n",
    "        \n",
    "        @self.app.route('/invoke/<graph_name>', methods=['POST'])\n",
    "        def invoke_graph(graph_name: str):\n",
    "            try:\n",
    "                data = request.json\n",
    "                request_id = str(uuid.uuid4())\n",
    "                \n",
    "                # 요청을 큐에 추가\n",
    "                self.request_queue.put({\n",
    "                    \"id\": request_id,\n",
    "                    \"graph\": graph_name,\n",
    "                    \"data\": data\n",
    "                })\n",
    "                \n",
    "                # 동기 실행 (실제로는 비동기 처리 권장)\n",
    "                result = self._process_request(graph_name, data)\n",
    "                \n",
    "                return jsonify({\n",
    "                    \"request_id\": request_id,\n",
    "                    \"status\": \"success\",\n",
    "                    \"result\": result\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e)\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/batch', methods=['POST'])\n",
    "        def batch_invoke():\n",
    "            \"\"\"배치 처리\"\"\"\n",
    "            try:\n",
    "                batch_requests = request.json.get(\"requests\", [])\n",
    "                results = []\n",
    "                \n",
    "                for req in batch_requests:\n",
    "                    graph_name = req.get(\"graph\")\n",
    "                    data = req.get(\"data\")\n",
    "                    \n",
    "                    try:\n",
    "                        result = self._process_request(graph_name, data)\n",
    "                        results.append({\n",
    "                            \"status\": \"success\",\n",
    "                            \"result\": result\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        results.append({\n",
    "                            \"status\": \"error\",\n",
    "                            \"error\": str(e)\n",
    "                        })\n",
    "                \n",
    "                return jsonify({\"results\": results})\n",
    "            except Exception as e:\n",
    "                return jsonify({\"error\": str(e)}), 500\n",
    "    \n",
    "    def _process_request(self, graph_name: str, data: Dict) -> Any:\n",
    "        \"\"\"요청 처리\"\"\"\n",
    "        # 실제로는 비동기 처리가 필요\n",
    "        import asyncio\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        result = loop.run_until_complete(\n",
    "            self.service.invoke_graph(graph_name, data)\n",
    "        )\n",
    "        loop.close()\n",
    "        return result\n",
    "    \n",
    "    def run(self, debug: bool = False):\n",
    "        \"\"\"서버 실행\"\"\"\n",
    "        print(f\"🚀 API 서버가 포트 {self.port}에서 시작됩니다...\")\n",
    "        self.app.run(port=self.port, debug=debug)\n",
    "\n",
    "# API 클라이언트 예제\n",
    "class LangGraphClient:\n",
    "    \"\"\"LangGraph API 클라이언트\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def health_check(self) -> Dict:\n",
    "        import requests\n",
    "        response = requests.get(f\"{self.base_url}/health\")\n",
    "        return response.json()\n",
    "    \n",
    "    def list_graphs(self) -> List[str]:\n",
    "        import requests\n",
    "        response = requests.get(f\"{self.base_url}/graphs\")\n",
    "        return response.json()[\"graphs\"]\n",
    "    \n",
    "    def invoke(self, graph_name: str, data: Dict) -> Dict:\n",
    "        import requests\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/invoke/{graph_name}\",\n",
    "            json=data\n",
    "        )\n",
    "        return response.json()\n",
    "    \n",
    "    def batch_invoke(self, requests: List[Dict]) -> List[Dict]:\n",
    "        import requests\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/batch\",\n",
    "            json={\"requests\": requests}\n",
    "        )\n",
    "        return response.json()[\"results\"]\n",
    "\n",
    "# API 서버 시뮬레이션\n",
    "def simulate_api_server():\n",
    "    \"\"\"API 서버 시뮬레이션\"\"\"\n",
    "    print(\"API 서버 시뮬레이션\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 설정 및 서비스 생성\n",
    "    config = create_environment_config(\"development\")\n",
    "    service = GraphDeploymentService(config)\n",
    "    \n",
    "    # 그래프 등록\n",
    "    service.register_graph(\"chat\", build_chat_graph)\n",
    "    service.register_graph(\"analysis\", build_analysis_graph)\n",
    "    \n",
    "    # API 서버 생성 (실제 실행하지 않음)\n",
    "    api_server = LangGraphAPIServer(service, port=5000)\n",
    "    \n",
    "    print(\"\\n📋 API 엔드포인트:\")\n",
    "    print(\"- GET  /health       : 헬스 체크\")\n",
    "    print(\"- GET  /graphs       : 그래프 목록\")\n",
    "    print(\"- POST /invoke/<name>: 그래프 실행\")\n",
    "    print(\"- POST /batch        : 배치 처리\")\n",
    "    \n",
    "    print(\"\\n💡 서버 실행 명령:\")\n",
    "    print(\"api_server.run(debug=True)\")\n",
    "    \n",
    "    # 클라이언트 사용 예제\n",
    "    print(\"\\n📱 클라이언트 사용 예제:\")\n",
    "    print(\"\"\"\n",
    "client = LangGraphClient(\"http://localhost:5000\")\n",
    "\n",
    "# 헬스 체크\n",
    "health = client.health_check()\n",
    "print(health)\n",
    "\n",
    "# 그래프 실행\n",
    "result = client.invoke(\"chat\", {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "})\n",
    "print(result)\n",
    "    \"\"\")\n",
    "\n",
    "simulate_api_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Docker Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker 배포 설정 생성\n",
    "def create_dockerfile() -> str:\n",
    "    \"\"\"Dockerfile 생성\"\"\"\n",
    "    dockerfile_content = \"\"\"\n",
    "# Base image\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    gcc \\\\\n",
    "    g++ \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV LANGGRAPH_ENV=production\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\\\n",
    "    CMD python -c \"import requests; requests.get('http://localhost:8000/health')\" || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"python\", \"-m\", \"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "    return dockerfile_content.strip()\n",
    "\n",
    "def create_docker_compose() -> str:\n",
    "    \"\"\"Docker Compose 파일 생성\"\"\"\n",
    "    docker_compose_content = \"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  langgraph-app:\n",
    "    build: .\n",
    "    container_name: langgraph-service\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - OPENAI_API_KEY=${OPENAI_API_KEY}\n",
    "      - DATABASE_URL=postgresql://user:password@db:5432/langgraph\n",
    "      - REDIS_URL=redis://redis:6379\n",
    "      - LANGGRAPH_ENV=production\n",
    "    depends_on:\n",
    "      - db\n",
    "      - redis\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "    volumes:\n",
    "      - ./logs:/app/logs\n",
    "    \n",
    "  db:\n",
    "    image: postgres:15\n",
    "    container_name: langgraph-db\n",
    "    environment:\n",
    "      - POSTGRES_USER=user\n",
    "      - POSTGRES_PASSWORD=password\n",
    "      - POSTGRES_DB=langgraph\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "    \n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    container_name: langgraph-redis\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "  \n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    container_name: langgraph-nginx\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "      - \"443:443\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf\n",
    "      - ./ssl:/etc/nginx/ssl\n",
    "    depends_on:\n",
    "      - langgraph-app\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "\n",
    "networks:\n",
    "  langgraph-network:\n",
    "    driver: bridge\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  redis_data:\n",
    "\"\"\"\n",
    "    return docker_compose_content.strip()\n",
    "\n",
    "def create_kubernetes_deployment() -> str:\n",
    "    \"\"\"Kubernetes 배포 YAML 생성\"\"\"\n",
    "    k8s_deployment = \"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: langgraph-deployment\n",
    "  labels:\n",
    "    app: langgraph\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: langgraph\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: langgraph\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: langgraph\n",
    "        image: langgraph-app:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        env:\n",
    "        - name: OPENAI_API_KEY\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: langgraph-secrets\n",
    "              key: openai-api-key\n",
    "        - name: DATABASE_URL\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: langgraph-secrets\n",
    "              key: database-url\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"256Mi\"\n",
    "            cpu: \"250m\"\n",
    "          limits:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: langgraph-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: langgraph\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "\"\"\"\n",
    "    return k8s_deployment.strip()\n",
    "\n",
    "# 배포 파일 생성\n",
    "print(\"📦 Docker 배포 파일 생성\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dockerfile\n",
    "dockerfile = create_dockerfile()\n",
    "print(\"\\n1. Dockerfile:\")\n",
    "print(dockerfile[:300] + \"...\")\n",
    "\n",
    "# Docker Compose\n",
    "docker_compose = create_docker_compose()\n",
    "print(\"\\n2. docker-compose.yml:\")\n",
    "print(docker_compose[:400] + \"...\")\n",
    "\n",
    "# Kubernetes\n",
    "k8s_config = create_kubernetes_deployment()\n",
    "print(\"\\n3. kubernetes-deployment.yaml:\")\n",
    "print(k8s_config[:400] + \"...\")\n",
    "\n",
    "# 배포 명령어\n",
    "print(\"\\n🚀 배포 명령어:\")\n",
    "print(\"\"\"\n",
    "# Docker 빌드 및 실행\n",
    "docker build -t langgraph-app .\n",
    "docker run -p 8000:8000 --env-file .env langgraph-app\n",
    "\n",
    "# Docker Compose\n",
    "docker-compose up -d\n",
    "\n",
    "# Kubernetes\n",
    "kubectl apply -f kubernetes-deployment.yaml\n",
    "kubectl get pods -l app=langgraph\n",
    "kubectl get service langgraph-service\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitoring and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Any\n",
    "import traceback\n",
    "\n",
    "# 구조화된 로거\n",
    "class StructuredLogger:\n",
    "    \"\"\"구조화된 로깅 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, level: str = \"INFO\"):\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.logger.setLevel(getattr(logging, level))\n",
    "        \n",
    "        # JSON 포맷터\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        # 핸들러 설정\n",
    "        handler = logging.StreamHandler()\n",
    "        handler.setFormatter(formatter)\n",
    "        self.logger.addHandler(handler)\n",
    "    \n",
    "    def log_event(self, event_type: str, data: Dict[str, Any]):\n",
    "        \"\"\"이벤트 로깅\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"event_type\": event_type,\n",
    "            \"data\": data\n",
    "        }\n",
    "        self.logger.info(json.dumps(log_entry))\n",
    "    \n",
    "    def log_error(self, error: Exception, context: Dict[str, Any] = None):\n",
    "        \"\"\"에러 로깅\"\"\"\n",
    "        error_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"error_type\": type(error).__name__,\n",
    "            \"error_message\": str(error),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "            \"context\": context or {}\n",
    "        }\n",
    "        self.logger.error(json.dumps(error_entry))\n",
    "\n",
    "# 메트릭 수집기\n",
    "class MetricsCollector:\n",
    "    \"\"\"메트릭 수집 및 모니터링\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"counters\": {},\n",
    "            \"gauges\": {},\n",
    "            \"histograms\": {}\n",
    "        }\n",
    "    \n",
    "    def increment_counter(self, name: str, value: int = 1, labels: Dict = None):\n",
    "        \"\"\"카운터 증가\"\"\"\n",
    "        key = self._create_key(name, labels)\n",
    "        if key not in self.metrics[\"counters\"]:\n",
    "            self.metrics[\"counters\"][key] = 0\n",
    "        self.metrics[\"counters\"][key] += value\n",
    "    \n",
    "    def set_gauge(self, name: str, value: float, labels: Dict = None):\n",
    "        \"\"\"게이지 설정\"\"\"\n",
    "        key = self._create_key(name, labels)\n",
    "        self.metrics[\"gauges\"][key] = value\n",
    "    \n",
    "    def record_histogram(self, name: str, value: float, labels: Dict = None):\n",
    "        \"\"\"히스토그램 기록\"\"\"\n",
    "        key = self._create_key(name, labels)\n",
    "        if key not in self.metrics[\"histograms\"]:\n",
    "            self.metrics[\"histograms\"][key] = []\n",
    "        self.metrics[\"histograms\"][key].append(value)\n",
    "    \n",
    "    def _create_key(self, name: str, labels: Dict = None) -> str:\n",
    "        \"\"\"메트릭 키 생성\"\"\"\n",
    "        if labels:\n",
    "            label_str = \",\".join(f\"{k}={v}\" for k, v in sorted(labels.items()))\n",
    "            return f\"{name}{{{label_str}}}\"\n",
    "        return name\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"모든 메트릭 반환\"\"\"\n",
    "        return self.metrics\n",
    "    \n",
    "    def export_prometheus(self) -> str:\n",
    "        \"\"\"Prometheus 포맷으로 내보내기\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Counters\n",
    "        for key, value in self.metrics[\"counters\"].items():\n",
    "            lines.append(f\"# TYPE {key.split('{')[0]} counter\")\n",
    "            lines.append(f\"{key} {value}\")\n",
    "        \n",
    "        # Gauges\n",
    "        for key, value in self.metrics[\"gauges\"].items():\n",
    "            lines.append(f\"# TYPE {key.split('{')[0]} gauge\")\n",
    "            lines.append(f\"{key} {value}\")\n",
    "        \n",
    "        # Histograms (simplified)\n",
    "        for key, values in self.metrics[\"histograms\"].items():\n",
    "            if values:\n",
    "                lines.append(f\"# TYPE {key.split('{')[0]} histogram\")\n",
    "                lines.append(f\"{key}_count {len(values)}\")\n",
    "                lines.append(f\"{key}_sum {sum(values)}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# 모니터링 데코레이터\n",
    "def monitor_performance(logger: StructuredLogger, collector: MetricsCollector):\n",
    "    \"\"\"성능 모니터링 데코레이터\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # 함수 실행\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                # 성공 메트릭\n",
    "                latency = time.time() - start_time\n",
    "                collector.increment_counter(\n",
    "                    \"function_calls_total\",\n",
    "                    labels={\"function\": func.__name__, \"status\": \"success\"}\n",
    "                )\n",
    "                collector.record_histogram(\n",
    "                    \"function_duration_seconds\",\n",
    "                    latency,\n",
    "                    labels={\"function\": func.__name__}\n",
    "                )\n",
    "                \n",
    "                # 로깅\n",
    "                logger.log_event(\"function_executed\", {\n",
    "                    \"function\": func.__name__,\n",
    "                    \"duration\": latency,\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                # 실패 메트릭\n",
    "                collector.increment_counter(\n",
    "                    \"function_calls_total\",\n",
    "                    labels={\"function\": func.__name__, \"status\": \"error\"}\n",
    "                )\n",
    "                \n",
    "                # 에러 로깅\n",
    "                logger.log_error(e, {\"function\": func.__name__})\n",
    "                raise\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# 모니터링 시스템 테스트\n",
    "logger = StructuredLogger(\"langgraph_app\")\n",
    "collector = MetricsCollector()\n",
    "\n",
    "# 모니터링이 적용된 함수\n",
    "@monitor_performance(logger, collector)\n",
    "def process_graph_request(graph_name: str, data: Dict):\n",
    "    \"\"\"그래프 요청 처리\"\"\"\n",
    "    import random\n",
    "    time.sleep(random.uniform(0.1, 0.5))  # 시뮬레이션\n",
    "    \n",
    "    if random.random() < 0.1:  # 10% 실패율\n",
    "        raise Exception(\"Random processing error\")\n",
    "    \n",
    "    return {\"result\": \"success\", \"graph\": graph_name}\n",
    "\n",
    "# 테스트 실행\n",
    "print(\"📊 모니터링 시스템 테스트\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        result = process_graph_request(\"chat\", {\"message\": f\"test_{i}\"})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 메트릭 출력\n",
    "print(\"\\n📈 수집된 메트릭:\")\n",
    "metrics = collector.get_metrics()\n",
    "print(f\"Counters: {metrics['counters']}\")\n",
    "print(f\"Histograms: {len(metrics['histograms'])} entries\")\n",
    "\n",
    "print(\"\\n📋 Prometheus 포맷:\")\n",
    "print(collector.export_prometheus()[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 과제\n",
    "\n",
    "1. 실제 LangGraph 애플리케이션을 Docker로 컨테이너화하기\n",
    "2. CI/CD 파이프라인 구성하기 (GitHub Actions)\n",
    "3. 프로덕션 환경 모니터링 대시보드 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하세요\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}