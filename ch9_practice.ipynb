{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: LangGraph Deployment ì‹¤ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ LangGraph ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë°°í¬ì™€ í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ì„±ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LangGraph Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any, TypedDict, Optional\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangGraph ì„¤ì • í´ë˜ìŠ¤\n",
    "@dataclass\n",
    "class LangGraphConfig:\n",
    "    \"\"\"LangGraph ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •\"\"\"\n",
    "    app_name: str\n",
    "    version: str\n",
    "    environment: str  # development, staging, production\n",
    "    api_keys: Dict[str, str]\n",
    "    database_url: Optional[str] = None\n",
    "    redis_url: Optional[str] = None\n",
    "    max_workers: int = 4\n",
    "    timeout: int = 30\n",
    "    retry_policy: Dict[str, Any] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"app_name\": self.app_name,\n",
    "            \"version\": self.version,\n",
    "            \"environment\": self.environment,\n",
    "            \"max_workers\": self.max_workers,\n",
    "            \"timeout\": self.timeout,\n",
    "            \"retry_policy\": self.retry_policy or {\n",
    "                \"max_retries\": 3,\n",
    "                \"backoff_factor\": 2,\n",
    "                \"max_backoff\": 60\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def save_to_file(self, filepath: str):\n",
    "        \"\"\"ì„¤ì •ì„ íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        config_dict = self.to_dict()\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "        print(f\"ì„¤ì •ì´ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_file(cls, filepath: str):\n",
    "        \"\"\"íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        return cls(**config_dict)\n",
    "\n",
    "# í™˜ê²½ë³„ ì„¤ì • ìƒì„±\n",
    "def create_environment_config(env: str) -> LangGraphConfig:\n",
    "    \"\"\"í™˜ê²½ë³„ ì„¤ì • ìƒì„±\"\"\"\n",
    "    base_config = {\n",
    "        \"app_name\": \"langgraph-app\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"api_keys\": {\n",
    "            \"openai\": os.getenv(\"OPENAI_API_KEY\", \"\"),\n",
    "            \"anthropic\": os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if env == \"development\":\n",
    "        return LangGraphConfig(\n",
    "            **base_config,\n",
    "            environment=\"development\",\n",
    "            database_url=\"sqlite:///dev.db\",\n",
    "            max_workers=2,\n",
    "            timeout=60\n",
    "        )\n",
    "    elif env == \"staging\":\n",
    "        return LangGraphConfig(\n",
    "            **base_config,\n",
    "            environment=\"staging\",\n",
    "            database_url=os.getenv(\"DATABASE_URL\"),\n",
    "            redis_url=os.getenv(\"REDIS_URL\"),\n",
    "            max_workers=4,\n",
    "            timeout=30\n",
    "        )\n",
    "    elif env == \"production\":\n",
    "        return LangGraphConfig(\n",
    "            **base_config,\n",
    "            environment=\"production\",\n",
    "            database_url=os.getenv(\"DATABASE_URL\"),\n",
    "            redis_url=os.getenv(\"REDIS_URL\"),\n",
    "            max_workers=8,\n",
    "            timeout=30,\n",
    "            retry_policy={\n",
    "                \"max_retries\": 5,\n",
    "                \"backoff_factor\": 2,\n",
    "                \"max_backoff\": 120\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    return LangGraphConfig(**base_config, environment=\"development\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "dev_config = create_environment_config(\"development\")\n",
    "prod_config = create_environment_config(\"production\")\n",
    "\n",
    "print(\"ê°œë°œ í™˜ê²½ ì„¤ì •:\")\n",
    "print(json.dumps(dev_config.to_dict(), indent=2))\n",
    "\n",
    "print(\"\\ní”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì •:\")\n",
    "print(json.dumps(prod_config.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Deployment Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ë°°í¬ ê°€ëŠ¥í•œ ê·¸ë˜í”„ ì„œë¹„ìŠ¤\n",
    "class GraphDeploymentService:\n",
    "    \"\"\"LangGraph ë°°í¬ ì„œë¹„ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, config: LangGraphConfig):\n",
    "        self.config = config\n",
    "        self.graphs = {}\n",
    "        self.metrics = {\n",
    "            \"requests\": 0,\n",
    "            \"errors\": 0,\n",
    "            \"avg_latency\": 0,\n",
    "            \"start_time\": datetime.now()\n",
    "        }\n",
    "    \n",
    "    def register_graph(self, name: str, graph_builder: Callable):\n",
    "        \"\"\"ê·¸ë˜í”„ ë“±ë¡\"\"\"\n",
    "        try:\n",
    "            graph = graph_builder()\n",
    "            self.graphs[name] = graph\n",
    "            print(f\"âœ… ê·¸ë˜í”„ '{name}' ë“±ë¡ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ê·¸ë˜í”„ '{name}' ë“±ë¡ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    async def invoke_graph(self, graph_name: str, input_data: Dict) -> Dict:\n",
    "        \"\"\"ê·¸ë˜í”„ ì‹¤í–‰\"\"\"\n",
    "        if graph_name not in self.graphs:\n",
    "            raise ValueError(f\"ê·¸ë˜í”„ '{graph_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.metrics[\"requests\"] += 1\n",
    "        \n",
    "        try:\n",
    "            # íƒ€ì„ì•„ì›ƒ ì ìš©\n",
    "            result = await asyncio.wait_for(\n",
    "                self._execute_graph(graph_name, input_data),\n",
    "                timeout=self.config.timeout\n",
    "            )\n",
    "            \n",
    "            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸\n",
    "            latency = time.time() - start_time\n",
    "            self._update_metrics(latency)\n",
    "            \n",
    "            return result\n",
    "        except asyncio.TimeoutError:\n",
    "            self.metrics[\"errors\"] += 1\n",
    "            raise TimeoutError(f\"ê·¸ë˜í”„ ì‹¤í–‰ì´ {self.config.timeout}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            self.metrics[\"errors\"] += 1\n",
    "            raise e\n",
    "    \n",
    "    async def _execute_graph(self, graph_name: str, input_data: Dict) -> Dict:\n",
    "        \"\"\"ì‹¤ì œ ê·¸ë˜í”„ ì‹¤í–‰\"\"\"\n",
    "        graph = self.graphs[graph_name]\n",
    "        # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰\n",
    "        loop = asyncio.get_event_loop()\n",
    "        result = await loop.run_in_executor(None, graph.invoke, input_data)\n",
    "        return result\n",
    "    \n",
    "    def _update_metrics(self, latency: float):\n",
    "        \"\"\"ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        current_avg = self.metrics[\"avg_latency\"]\n",
    "        total_requests = self.metrics[\"requests\"]\n",
    "        self.metrics[\"avg_latency\"] = (\n",
    "            (current_avg * (total_requests - 1) + latency) / total_requests\n",
    "        )\n",
    "    \n",
    "    def get_health_status(self) -> Dict:\n",
    "        \"\"\"í—¬ìŠ¤ ì²´í¬\"\"\"\n",
    "        uptime = (datetime.now() - self.metrics[\"start_time\"]).total_seconds()\n",
    "        error_rate = (\n",
    "            self.metrics[\"errors\"] / self.metrics[\"requests\"]\n",
    "            if self.metrics[\"requests\"] > 0 else 0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"healthy\" if error_rate < 0.1 else \"degraded\",\n",
    "            \"uptime_seconds\": uptime,\n",
    "            \"total_requests\": self.metrics[\"requests\"],\n",
    "            \"error_rate\": error_rate,\n",
    "            \"avg_latency_ms\": self.metrics[\"avg_latency\"] * 1000,\n",
    "            \"registered_graphs\": list(self.graphs.keys())\n",
    "        }\n",
    "\n",
    "# ìƒ˜í”Œ ê·¸ë˜í”„ ë¹Œë”\n",
    "def build_chat_graph():\n",
    "    \"\"\"ì±„íŒ… ê·¸ë˜í”„ ìƒì„±\"\"\"\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    \n",
    "    def chat_node(state: MessagesState):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    workflow.add_node(\"chat\", chat_node)\n",
    "    workflow.set_entry_point(\"chat\")\n",
    "    workflow.add_edge(\"chat\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def build_analysis_graph():\n",
    "    \"\"\"ë¶„ì„ ê·¸ë˜í”„ ìƒì„±\"\"\"\n",
    "    class AnalysisState(TypedDict):\n",
    "        text: str\n",
    "        analysis: str\n",
    "    \n",
    "    workflow = StateGraph(AnalysisState)\n",
    "    \n",
    "    def analyze_node(state: AnalysisState):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        prompt = f\"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”: {state['text']}\"\n",
    "        response = llm.invoke(prompt)\n",
    "        return {\"analysis\": response.content}\n",
    "    \n",
    "    workflow.add_node(\"analyze\", analyze_node)\n",
    "    workflow.set_entry_point(\"analyze\")\n",
    "    workflow.add_edge(\"analyze\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# ì„œë¹„ìŠ¤ ë°°í¬ ì‹œë®¬ë ˆì´ì…˜\n",
    "async def deploy_and_test():\n",
    "    \"\"\"ë°°í¬ ë° í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    # ì„¤ì • ìƒì„±\n",
    "    config = create_environment_config(\"staging\")\n",
    "    \n",
    "    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”\n",
    "    service = GraphDeploymentService(config)\n",
    "    \n",
    "    # ê·¸ë˜í”„ ë“±ë¡\n",
    "    service.register_graph(\"chat\", build_chat_graph)\n",
    "    service.register_graph(\"analysis\", build_analysis_graph)\n",
    "    \n",
    "    print(\"\\nğŸ“Š ì„œë¹„ìŠ¤ ìƒíƒœ:\")\n",
    "    print(json.dumps(service.get_health_status(), indent=2))\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ìš”ì²­\n",
    "    print(\"\\nğŸš€ í…ŒìŠ¤íŠ¸ ìš”ì²­ ì‹¤í–‰:\")\n",
    "    \n",
    "    # ì±„íŒ… ê·¸ë˜í”„ í…ŒìŠ¤íŠ¸\n",
    "    chat_result = await service.invoke_graph(\n",
    "        \"chat\",\n",
    "        {\"messages\": [HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”!\")]}\n",
    "    )\n",
    "    print(f\"ì±„íŒ… ì‘ë‹µ: {chat_result['messages'][-1].content[:50]}...\")\n",
    "    \n",
    "    # ë¶„ì„ ê·¸ë˜í”„ í…ŒìŠ¤íŠ¸\n",
    "    analysis_result = await service.invoke_graph(\n",
    "        \"analysis\",\n",
    "        {\"text\": \"LangGraphëŠ” ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\"}\n",
    "    )\n",
    "    print(f\"ë¶„ì„ ê²°ê³¼: {analysis_result['analysis'][:50]}...\")\n",
    "    \n",
    "    # ìµœì¢… ë©”íŠ¸ë¦­\n",
    "    print(\"\\nğŸ“ˆ ìµœì¢… ë©”íŠ¸ë¦­:\")\n",
    "    print(json.dumps(service.get_health_status(), indent=2))\n",
    "\n",
    "# ì‹¤í–‰\n",
    "await deploy_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. API Server Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import uuid\n",
    "from queue import Queue\n",
    "from typing import Any\n",
    "\n",
    "# API ì„œë²„ êµ¬í˜„\n",
    "class LangGraphAPIServer:\n",
    "    \"\"\"LangGraph API ì„œë²„\"\"\"\n",
    "    \n",
    "    def __init__(self, service: GraphDeploymentService, port: int = 5000):\n",
    "        self.service = service\n",
    "        self.port = port\n",
    "        self.app = Flask(__name__)\n",
    "        CORS(self.app)\n",
    "        self.request_queue = Queue()\n",
    "        self.response_cache = {}\n",
    "        self._setup_routes()\n",
    "    \n",
    "    def _setup_routes(self):\n",
    "        \"\"\"API ë¼ìš°íŠ¸ ì„¤ì •\"\"\"\n",
    "        \n",
    "        @self.app.route('/health', methods=['GET'])\n",
    "        def health_check():\n",
    "            return jsonify(self.service.get_health_status())\n",
    "        \n",
    "        @self.app.route('/graphs', methods=['GET'])\n",
    "        def list_graphs():\n",
    "            return jsonify({\n",
    "                \"graphs\": list(self.service.graphs.keys()),\n",
    "                \"count\": len(self.service.graphs)\n",
    "            })\n",
    "        \n",
    "        @self.app.route('/invoke/<graph_name>', methods=['POST'])\n",
    "        def invoke_graph(graph_name: str):\n",
    "            try:\n",
    "                data = request.json\n",
    "                request_id = str(uuid.uuid4())\n",
    "                \n",
    "                # ìš”ì²­ì„ íì— ì¶”ê°€\n",
    "                self.request_queue.put({\n",
    "                    \"id\": request_id,\n",
    "                    \"graph\": graph_name,\n",
    "                    \"data\": data\n",
    "                })\n",
    "                \n",
    "                # ë™ê¸° ì‹¤í–‰ (ì‹¤ì œë¡œëŠ” ë¹„ë™ê¸° ì²˜ë¦¬ ê¶Œì¥)\n",
    "                result = self._process_request(graph_name, data)\n",
    "                \n",
    "                return jsonify({\n",
    "                    \"request_id\": request_id,\n",
    "                    \"status\": \"success\",\n",
    "                    \"result\": result\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e)\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/batch', methods=['POST'])\n",
    "        def batch_invoke():\n",
    "            \"\"\"ë°°ì¹˜ ì²˜ë¦¬\"\"\"\n",
    "            try:\n",
    "                batch_requests = request.json.get(\"requests\", [])\n",
    "                results = []\n",
    "                \n",
    "                for req in batch_requests:\n",
    "                    graph_name = req.get(\"graph\")\n",
    "                    data = req.get(\"data\")\n",
    "                    \n",
    "                    try:\n",
    "                        result = self._process_request(graph_name, data)\n",
    "                        results.append({\n",
    "                            \"status\": \"success\",\n",
    "                            \"result\": result\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        results.append({\n",
    "                            \"status\": \"error\",\n",
    "                            \"error\": str(e)\n",
    "                        })\n",
    "                \n",
    "                return jsonify({\"results\": results})\n",
    "            except Exception as e:\n",
    "                return jsonify({\"error\": str(e)}), 500\n",
    "    \n",
    "    def _process_request(self, graph_name: str, data: Dict) -> Any:\n",
    "        \"\"\"ìš”ì²­ ì²˜ë¦¬\"\"\"\n",
    "        # ì‹¤ì œë¡œëŠ” ë¹„ë™ê¸° ì²˜ë¦¬ê°€ í•„ìš”\n",
    "        import asyncio\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        result = loop.run_until_complete(\n",
    "            self.service.invoke_graph(graph_name, data)\n",
    "        )\n",
    "        loop.close()\n",
    "        return result\n",
    "    \n",
    "    def run(self, debug: bool = False):\n",
    "        \"\"\"ì„œë²„ ì‹¤í–‰\"\"\"\n",
    "        print(f\"ğŸš€ API ì„œë²„ê°€ í¬íŠ¸ {self.port}ì—ì„œ ì‹œì‘ë©ë‹ˆë‹¤...\")\n",
    "        self.app.run(port=self.port, debug=debug)\n",
    "\n",
    "# API í´ë¼ì´ì–¸íŠ¸ ì˜ˆì œ\n",
    "class LangGraphClient:\n",
    "    \"\"\"LangGraph API í´ë¼ì´ì–¸íŠ¸\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def health_check(self) -> Dict:\n",
    "        import requests\n",
    "        response = requests.get(f\"{self.base_url}/health\")\n",
    "        return response.json()\n",
    "    \n",
    "    def list_graphs(self) -> List[str]:\n",
    "        import requests\n",
    "        response = requests.get(f\"{self.base_url}/graphs\")\n",
    "        return response.json()[\"graphs\"]\n",
    "    \n",
    "    def invoke(self, graph_name: str, data: Dict) -> Dict:\n",
    "        import requests\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/invoke/{graph_name}\",\n",
    "            json=data\n",
    "        )\n",
    "        return response.json()\n",
    "    \n",
    "    def batch_invoke(self, requests: List[Dict]) -> List[Dict]:\n",
    "        import requests\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/batch\",\n",
    "            json={\"requests\": requests}\n",
    "        )\n",
    "        return response.json()[\"results\"]\n",
    "\n",
    "# API ì„œë²„ ì‹œë®¬ë ˆì´ì…˜\n",
    "def simulate_api_server():\n",
    "    \"\"\"API ì„œë²„ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "    print(\"API ì„œë²„ ì‹œë®¬ë ˆì´ì…˜\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ì„¤ì • ë° ì„œë¹„ìŠ¤ ìƒì„±\n",
    "    config = create_environment_config(\"development\")\n",
    "    service = GraphDeploymentService(config)\n",
    "    \n",
    "    # ê·¸ë˜í”„ ë“±ë¡\n",
    "    service.register_graph(\"chat\", build_chat_graph)\n",
    "    service.register_graph(\"analysis\", build_analysis_graph)\n",
    "    \n",
    "    # API ì„œë²„ ìƒì„± (ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ)\n",
    "    api_server = LangGraphAPIServer(service, port=5000)\n",
    "    \n",
    "    print(\"\\nğŸ“‹ API ì—”ë“œí¬ì¸íŠ¸:\")\n",
    "    print(\"- GET  /health       : í—¬ìŠ¤ ì²´í¬\")\n",
    "    print(\"- GET  /graphs       : ê·¸ë˜í”„ ëª©ë¡\")\n",
    "    print(\"- POST /invoke/<name>: ê·¸ë˜í”„ ì‹¤í–‰\")\n",
    "    print(\"- POST /batch        : ë°°ì¹˜ ì²˜ë¦¬\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ ì„œë²„ ì‹¤í–‰ ëª…ë ¹:\")\n",
    "    print(\"api_server.run(debug=True)\")\n",
    "    \n",
    "    # í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì˜ˆì œ\n",
    "    print(\"\\nğŸ“± í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì˜ˆì œ:\")\n",
    "    print(\"\"\"\n",
    "client = LangGraphClient(\"http://localhost:5000\")\n",
    "\n",
    "# í—¬ìŠ¤ ì²´í¬\n",
    "health = client.health_check()\n",
    "print(health)\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "result = client.invoke(\"chat\", {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "})\n",
    "print(result)\n",
    "    \"\"\")\n",
    "\n",
    "simulate_api_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Docker Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker ë°°í¬ ì„¤ì • ìƒì„±\n",
    "def create_dockerfile() -> str:\n",
    "    \"\"\"Dockerfile ìƒì„±\"\"\"\n",
    "    dockerfile_content = \"\"\"\n",
    "# Base image\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    gcc \\\\\n",
    "    g++ \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV LANGGRAPH_ENV=production\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\\\n",
    "    CMD python -c \"import requests; requests.get('http://localhost:8000/health')\" || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"python\", \"-m\", \"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "    return dockerfile_content.strip()\n",
    "\n",
    "def create_docker_compose() -> str:\n",
    "    \"\"\"Docker Compose íŒŒì¼ ìƒì„±\"\"\"\n",
    "    docker_compose_content = \"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  langgraph-app:\n",
    "    build: .\n",
    "    container_name: langgraph-service\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - OPENAI_API_KEY=${OPENAI_API_KEY}\n",
    "      - DATABASE_URL=postgresql://user:password@db:5432/langgraph\n",
    "      - REDIS_URL=redis://redis:6379\n",
    "      - LANGGRAPH_ENV=production\n",
    "    depends_on:\n",
    "      - db\n",
    "      - redis\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "    volumes:\n",
    "      - ./logs:/app/logs\n",
    "    \n",
    "  db:\n",
    "    image: postgres:15\n",
    "    container_name: langgraph-db\n",
    "    environment:\n",
    "      - POSTGRES_USER=user\n",
    "      - POSTGRES_PASSWORD=password\n",
    "      - POSTGRES_DB=langgraph\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "    \n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    container_name: langgraph-redis\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "  \n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    container_name: langgraph-nginx\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "      - \"443:443\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf\n",
    "      - ./ssl:/etc/nginx/ssl\n",
    "    depends_on:\n",
    "      - langgraph-app\n",
    "    networks:\n",
    "      - langgraph-network\n",
    "\n",
    "networks:\n",
    "  langgraph-network:\n",
    "    driver: bridge\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  redis_data:\n",
    "\"\"\"\n",
    "    return docker_compose_content.strip()\n",
    "\n",
    "def create_kubernetes_deployment() -> str:\n",
    "    \"\"\"Kubernetes ë°°í¬ YAML ìƒì„±\"\"\"\n",
    "    k8s_deployment = \"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: langgraph-deployment\n",
    "  labels:\n",
    "    app: langgraph\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: langgraph\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: langgraph\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: langgraph\n",
    "        image: langgraph-app:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        env:\n",
    "        - name: OPENAI_API_KEY\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: langgraph-secrets\n",
    "              key: openai-api-key\n",
    "        - name: DATABASE_URL\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: langgraph-secrets\n",
    "              key: database-url\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"256Mi\"\n",
    "            cpu: \"250m\"\n",
    "          limits:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: langgraph-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: langgraph\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "\"\"\"\n",
    "    return k8s_deployment.strip()\n",
    "\n",
    "# ë°°í¬ íŒŒì¼ ìƒì„±\n",
    "print(\"ğŸ“¦ Docker ë°°í¬ íŒŒì¼ ìƒì„±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dockerfile\n",
    "dockerfile = create_dockerfile()\n",
    "print(\"\\n1. Dockerfile:\")\n",
    "print(dockerfile[:300] + \"...\")\n",
    "\n",
    "# Docker Compose\n",
    "docker_compose = create_docker_compose()\n",
    "print(\"\\n2. docker-compose.yml:\")\n",
    "print(docker_compose[:400] + \"...\")\n",
    "\n",
    "# Kubernetes\n",
    "k8s_config = create_kubernetes_deployment()\n",
    "print(\"\\n3. kubernetes-deployment.yaml:\")\n",
    "print(k8s_config[:400] + \"...\")\n",
    "\n",
    "# ë°°í¬ ëª…ë ¹ì–´\n",
    "print(\"\\nğŸš€ ë°°í¬ ëª…ë ¹ì–´:\")\n",
    "print(\"\"\"\n",
    "# Docker ë¹Œë“œ ë° ì‹¤í–‰\n",
    "docker build -t langgraph-app .\n",
    "docker run -p 8000:8000 --env-file .env langgraph-app\n",
    "\n",
    "# Docker Compose\n",
    "docker-compose up -d\n",
    "\n",
    "# Kubernetes\n",
    "kubectl apply -f kubernetes-deployment.yaml\n",
    "kubectl get pods -l app=langgraph\n",
    "kubectl get service langgraph-service\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitoring and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Any\n",
    "import traceback\n",
    "\n",
    "# êµ¬ì¡°í™”ëœ ë¡œê±°\n",
    "class StructuredLogger:\n",
    "    \"\"\"êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, level: str = \"INFO\"):\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.logger.setLevel(getattr(logging, level))\n",
    "        \n",
    "        # JSON í¬ë§·í„°\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        # í•¸ë“¤ëŸ¬ ì„¤ì •\n",
    "        handler = logging.StreamHandler()\n",
    "        handler.setFormatter(formatter)\n",
    "        self.logger.addHandler(handler)\n",
    "    \n",
    "    def log_event(self, event_type: str, data: Dict[str, Any]):\n",
    "        \"\"\"ì´ë²¤íŠ¸ ë¡œê¹…\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"event_type\": event_type,\n",
    "            \"data\": data\n",
    "        }\n",
    "        self.logger.info(json.dumps(log_entry))\n",
    "    \n",
    "    def log_error(self, error: Exception, context: Dict[str, Any] = None):\n",
    "        \"\"\"ì—ëŸ¬ ë¡œê¹…\"\"\"\n",
    "        error_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"error_type\": type(error).__name__,\n",
    "            \"error_message\": str(error),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "            \"context\": context or {}\n",
    "        }\n",
    "        self.logger.error(json.dumps(error_entry))\n",
    "\n",
    "# ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°\n",
    "class MetricsCollector:\n",
    "    \"\"\"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"counters\": {},\n",
    "            \"gauges\": {},\n",
    "            \"histograms\": {}\n",
    "        }\n",
    "    \n",
    "    def increment_counter(self, name: str, value: int = 1, labels: Dict = None):\n",
    "        \"\"\"ì¹´ìš´í„° ì¦ê°€\"\"\"\n",
    "        key = self._create_key(name, labels)\n",
    "        if key not in self.metrics[\"counters\"]:\n",
    "            self.metrics[\"counters\"][key] = 0\n",
    "        self.metrics[\"counters\"][key] += value\n",
    "    \n",
    "    def set_gauge(self, name: str, value: float, labels: Dict = None):\n",
    "        \"\"\"ê²Œì´ì§€ ì„¤ì •\"\"\"\n",
    "        key = self._create_key(name, labels)\n",
    "        self.metrics[\"gauges\"][key] = value\n",
    "    \n",
    "    def record_histogram(self, name: str, value: float, labels: Dict = None):\n",
    "        \"\"\"íˆìŠ¤í† ê·¸ë¨ ê¸°ë¡\"\"\"\n",
    "        key = self._create_key(name, labels)\n",
    "        if key not in self.metrics[\"histograms\"]:\n",
    "            self.metrics[\"histograms\"][key] = []\n",
    "        self.metrics[\"histograms\"][key].append(value)\n",
    "    \n",
    "    def _create_key(self, name: str, labels: Dict = None) -> str:\n",
    "        \"\"\"ë©”íŠ¸ë¦­ í‚¤ ìƒì„±\"\"\"\n",
    "        if labels:\n",
    "            label_str = \",\".join(f\"{k}={v}\" for k, v in sorted(labels.items()))\n",
    "            return f\"{name}{{{label_str}}}\"\n",
    "        return name\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"ëª¨ë“  ë©”íŠ¸ë¦­ ë°˜í™˜\"\"\"\n",
    "        return self.metrics\n",
    "    \n",
    "    def export_prometheus(self) -> str:\n",
    "        \"\"\"Prometheus í¬ë§·ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Counters\n",
    "        for key, value in self.metrics[\"counters\"].items():\n",
    "            lines.append(f\"# TYPE {key.split('{')[0]} counter\")\n",
    "            lines.append(f\"{key} {value}\")\n",
    "        \n",
    "        # Gauges\n",
    "        for key, value in self.metrics[\"gauges\"].items():\n",
    "            lines.append(f\"# TYPE {key.split('{')[0]} gauge\")\n",
    "            lines.append(f\"{key} {value}\")\n",
    "        \n",
    "        # Histograms (simplified)\n",
    "        for key, values in self.metrics[\"histograms\"].items():\n",
    "            if values:\n",
    "                lines.append(f\"# TYPE {key.split('{')[0]} histogram\")\n",
    "                lines.append(f\"{key}_count {len(values)}\")\n",
    "                lines.append(f\"{key}_sum {sum(values)}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°\n",
    "def monitor_performance(logger: StructuredLogger, collector: MetricsCollector):\n",
    "    \"\"\"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # í•¨ìˆ˜ ì‹¤í–‰\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                # ì„±ê³µ ë©”íŠ¸ë¦­\n",
    "                latency = time.time() - start_time\n",
    "                collector.increment_counter(\n",
    "                    \"function_calls_total\",\n",
    "                    labels={\"function\": func.__name__, \"status\": \"success\"}\n",
    "                )\n",
    "                collector.record_histogram(\n",
    "                    \"function_duration_seconds\",\n",
    "                    latency,\n",
    "                    labels={\"function\": func.__name__}\n",
    "                )\n",
    "                \n",
    "                # ë¡œê¹…\n",
    "                logger.log_event(\"function_executed\", {\n",
    "                    \"function\": func.__name__,\n",
    "                    \"duration\": latency,\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                # ì‹¤íŒ¨ ë©”íŠ¸ë¦­\n",
    "                collector.increment_counter(\n",
    "                    \"function_calls_total\",\n",
    "                    labels={\"function\": func.__name__, \"status\": \"error\"}\n",
    "                )\n",
    "                \n",
    "                # ì—ëŸ¬ ë¡œê¹…\n",
    "                logger.log_error(e, {\"function\": func.__name__})\n",
    "                raise\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "logger = StructuredLogger(\"langgraph_app\")\n",
    "collector = MetricsCollector()\n",
    "\n",
    "# ëª¨ë‹ˆí„°ë§ì´ ì ìš©ëœ í•¨ìˆ˜\n",
    "@monitor_performance(logger, collector)\n",
    "def process_graph_request(graph_name: str, data: Dict):\n",
    "    \"\"\"ê·¸ë˜í”„ ìš”ì²­ ì²˜ë¦¬\"\"\"\n",
    "    import random\n",
    "    time.sleep(random.uniform(0.1, 0.5))  # ì‹œë®¬ë ˆì´ì…˜\n",
    "    \n",
    "    if random.random() < 0.1:  # 10% ì‹¤íŒ¨ìœ¨\n",
    "        raise Exception(\"Random processing error\")\n",
    "    \n",
    "    return {\"result\": \"success\", \"graph\": graph_name}\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(\"ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        result = process_graph_request(\"chat\", {\"message\": f\"test_{i}\"})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ë©”íŠ¸ë¦­ ì¶œë ¥\n",
    "print(\"\\nğŸ“ˆ ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­:\")\n",
    "metrics = collector.get_metrics()\n",
    "print(f\"Counters: {metrics['counters']}\")\n",
    "print(f\"Histograms: {len(metrics['histograms'])} entries\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Prometheus í¬ë§·:\")\n",
    "print(collector.export_prometheus()[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "1. ì‹¤ì œ LangGraph ì• í”Œë¦¬ì¼€ì´ì…˜ì„ Dockerë¡œ ì»¨í…Œì´ë„ˆí™”í•˜ê¸°\n",
    "2. CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì„±í•˜ê¸° (GitHub Actions)\n",
    "3. í”„ë¡œë•ì…˜ í™˜ê²½ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì‹¤ìŠµ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}