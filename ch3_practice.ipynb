{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: RAG (Retrieval-Augmented Generation) 실습\n",
    "\n",
    "이 노트북은 다양한 RAG 기법과 전략을 실습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"OpenAI API Key를 입력하세요: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기본 RAG 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: RAG가 무엇인가요?\n",
      "답변: RAG는 \"검색 증강 생성\"의 약자로, 외부 지식을 활용하여 답변의 정확도를 높이는 방법론입니다. 이 접근 방식은 주어진 질문에 대해 관련 정보를 검색하고, 이를 바탕으로 더 정확하고 풍부한 답변을 생성하는 데 도움을 줍니다. RAG는 주로 자연어 처리(NLP) 분야에서 사용되며, 정보 검색과 생성 모델을 결합하여 성능을 향상시키는 데 기여합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 샘플 문서 준비\n",
    "documents = [\n",
    "    \"LangChain은 언어 모델을 활용한 애플리케이션 개발 프레임워크입니다.\",\n",
    "    \"RAG는 검색 증강 생성의 약자로, 외부 지식을 활용하여 답변의 정확도를 높입니다.\",\n",
    "    \"벡터 데이터베이스는 임베딩을 저장하고 유사도 검색을 수행합니다.\",\n",
    "    \"프롬프트 엔지니어링은 LLM에게 효과적인 지시를 제공하는 기술입니다.\",\n",
    "    \"체인은 여러 컴포넌트를 연결하여 복잡한 워크플로우를 구성합니다.\"\n",
    "]\n",
    "\n",
    "# 벡터 저장소 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# LLM 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# RAG 프롬프트\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"다음 컨텍스트를 바탕으로 질문에 답하세요:\\n\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# RAG 체인 구성\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 질문하기\n",
    "question = \"RAG가 무엇인가요?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "print(f\"질문: {question}\")\n",
    "print(f\"답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query Rewriting (질문 재작성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 질문: 그거 뭐야?\n",
      "재작성된 질문: \"그거\"가 무엇인지 구체적으로 알고 싶습니다. 어떤 주제나 사물에 대해 질문하시는 건가요? 예를 들어, 특정 제품, 개념, 사건 등에 대해 설명해 주시면 더 정확한 답변을 드릴 수 있습니다.\n",
      "\n",
      "답변: LangChain은 언어 모델을 활용하여 애플리케이션을 개발할 수 있도록 돕는 프레임워크입니다. 이 프레임워크는 여러 컴포넌트를 연결하여 복잡한 워크플로우를 구성할 수 있게 해주며, 이를 통해 다양한 언어 처리 작업을 효율적으로 수행할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 질문 재작성 프롬프트\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"사용자의 질문을 검색에 더 적합하도록 재작성하세요.\n",
    "    모호한 표현을 명확하게 하고, 핵심 키워드를 포함시키세요.\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 재작성 체인\n",
    "rewrite_chain = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 개선된 RAG 체인\n",
    "improved_rag_chain = (\n",
    "    {\"question\": rewrite_chain}\n",
    "    | {\"context\": lambda x: retriever.invoke(x[\"question\"]) | format_docs, \n",
    "       \"question\": lambda x: x[\"question\"]}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 테스트\n",
    "original_question = \"그거 뭐야?\"\n",
    "rewritten = rewrite_chain.invoke(original_question)\n",
    "print(f\"원본 질문: {original_question}\")\n",
    "print(f\"재작성된 질문: {rewritten}\")\n",
    "\n",
    "# RAG로 답변\n",
    "answer = rag_chain.invoke(\"LangChain이 뭐야?\")\n",
    "print(f\"\\n답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Query (다중 질문 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 질문: LangChain으로 무엇을 할 수 있나요?\n",
      "\n",
      "생성된 질문들:\n",
      "1. LangChain을 사용하여 자연어 처리(NLP) 작업을 어떻게 개선할 수 있나요?\n",
      "2. LangChain의 기능을 활용하여 데이터 분석에 어떤 이점을 제공할 수 있나요?\n",
      "3. LangChain을 이용한 챗봇 개발에서의 장점은 무엇인가요?\n",
      "\n",
      "검색된 문서 수: 3\n",
      "- LangChain은 언어 모델을 활용한 애플리케이션 개발 프레임워크입니다....\n",
      "- 프롬프트 엔지니어링은 LLM에게 효과적인 지시를 제공하는 기술입니다....\n",
      "- 체인은 여러 컴포넌트를 연결하여 복잡한 워크플로우를 구성합니다....\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 다중 질문 스키마\n",
    "class MultiQueries(BaseModel):\n",
    "    queries: List[str] = Field(description=\"생성된 질문 리스트\")\n",
    "\n",
    "# 다중 질문 생성 프롬프트\n",
    "multi_query_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"주어진 질문에 대해 다양한 관점에서 3개의 관련 질문을 생성하세요.\n",
    "    각 질문은 원본 질문의 다른 측면을 다루어야 합니다.\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 구조화된 출력으로 다중 질문 생성\n",
    "multi_query_llm = llm.with_structured_output(MultiQueries)\n",
    "multi_query_chain = multi_query_prompt | multi_query_llm\n",
    "\n",
    "# 다중 질문으로 검색 수행\n",
    "def multi_query_retrieval(question: str):\n",
    "    # 다중 질문 생성\n",
    "    multi_queries = multi_query_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # 각 질문으로 검색\n",
    "    all_docs = []\n",
    "    for query in multi_queries.queries:\n",
    "        docs = retriever.invoke(query)\n",
    "        all_docs.extend(docs)\n",
    "    \n",
    "    # 중복 제거\n",
    "    unique_docs = list({doc.page_content: doc for doc in all_docs}.values())\n",
    "    return unique_docs\n",
    "\n",
    "# 테스트\n",
    "question = \"LangChain으로 무엇을 할 수 있나요?\"\n",
    "queries = multi_query_chain.invoke({\"question\": question})\n",
    "print(f\"원본 질문: {question}\\n\")\n",
    "print(\"생성된 질문들:\")\n",
    "for i, q in enumerate(queries.queries, 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "\n",
    "# 다중 질문으로 검색\n",
    "docs = multi_query_retrieval(question)\n",
    "print(f\"\\n검색된 문서 수: {len(docs)}\")\n",
    "for doc in docs:\n",
    "    print(f\"- {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 벡터 데이터베이스의 역할은?\n",
      "\n",
      "RAG Fusion 결과:\n",
      "1. 벡터 데이터베이스는 임베딩을 저장하고 유사도 검색을 수행합니다.\n",
      "2. LangChain은 언어 모델을 활용한 애플리케이션 개발 프레임워크입니다.\n",
      "3. RAG는 검색 증강 생성의 약자로, 외부 지식을 활용하여 답변의 정확도를 높입니다.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def reciprocal_rank_fusion(search_results: List[List], k: int = 60) -> List[str]:\n",
    "    \"\"\"Reciprocal Rank Fusion 알고리즘\"\"\"\n",
    "    scores: Dict[str, float] = {}\n",
    "    \n",
    "    for result_list in search_results:\n",
    "        for rank, doc in enumerate(result_list, 1):\n",
    "            doc_id = doc.page_content\n",
    "            if doc_id not in scores:\n",
    "                scores[doc_id] = 0\n",
    "            scores[doc_id] += 1 / (rank + k)\n",
    "    \n",
    "    # 점수순으로 정렬\n",
    "    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in sorted_docs]\n",
    "\n",
    "# RAG Fusion 구현\n",
    "def rag_fusion(question: str, num_queries: int = 3):\n",
    "    # 1. 다중 질문 생성\n",
    "    multi_queries = multi_query_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # 2. 각 질문으로 검색\n",
    "    all_results = []\n",
    "    for query in multi_queries.queries[:num_queries]:\n",
    "        docs = retriever.invoke(query)\n",
    "        all_results.append(docs)\n",
    "    \n",
    "    # 3. RRF로 결과 융합\n",
    "    fused_results = reciprocal_rank_fusion(all_results)\n",
    "    \n",
    "    return fused_results[:3]  # 상위 3개 반환\n",
    "\n",
    "# 테스트\n",
    "question = \"벡터 데이터베이스의 역할은?\"\n",
    "fused_docs = rag_fusion(question)\n",
    "\n",
    "print(f\"질문: {question}\\n\")\n",
    "print(\"RAG Fusion 결과:\")\n",
    "for i, doc in enumerate(fused_docs, 1):\n",
    "    print(f\"{i}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HyDE (Hypothetical Document Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 프롬프트 엔지니어링의 중요성은?\n",
      "\n",
      "생성된 가상 문서:\n",
      "프롬프트 엔지니어링(Prompt Engineering)은 인공지능 모델, 특히 자연어 처리(NLP) 모델과의 상호작용에서 매우 중요한 역할을 합니다. 이는 사용자가 원하는 결과를 얻기 위해 모델에 제공하는 입력(프롬프트)을 설계하고 최적화하는 과정을 의미합니다. 다음은 프롬프트 엔지니어링의 중요성을 구체적으로 설명하는 몇 가지 이유입니다.\n",
      "\n",
      "### 1. 모델 성능 극대화\n",
      "프롬프트 엔지니어링은 모델의 성능을 극대화하는 데 필수적입니다. 적절한 프롬프트를 사용하면 모델이 더 정확하고 관련성 높은 응답을 생성할 수 있습니다. 예를 들어, 특정 질문에 대한 명확한 지침을 제공함으로써 모델이 더 나은 이해를 바탕으로 답변을 생성하도록 유도할 수 있습니다.\n",
      "\n",
      "### 2. 사용자 경험 향상\n",
      "사용자가 인공지능 모델과 상호작용할 때, 프롬프트의 품질은 최종 결과물의 품질에 직접적인 영향을 미칩니다. 잘 설계된 프롬프트는 사용자가 원하는 정보를 더 쉽게 얻을 수 있도록 도와주며, 이는 전반적인 사용자 경험을 향상시킵니다. 예를 들어, 명확하고 구체적인 질문을 통해 사용자는 더 유용한 정보를 얻을 수 있습니다.\n",
      "\n",
      "### 3. 다양한 응용 가능성\n",
      "프롬프트 엔지니어링은 다양한 분야에서 응용될 수 있습니다. 예를 들어, 고객 지원, 콘텐츠 생성, 데이터 분석 등 여러 산업에서 프롬프트를 최적화함으로써 특정 요구 사항에 맞는 결과를 도출할 수 있습니다. 이는 기업이 인공지능 기술을 활용하여 경쟁력을 높이는 데 기여합니다.\n",
      "\n",
      "### 4. 모델의 한계 이해\n",
      "프롬프트 엔지니어링을 통해 사용자는 모델의 한계를 이해하고 이를 극복할 수 있는 방법을 모색할 수 있습니다. 특정 프롬프트가 예상치 못한 결과를 초래할 경우, 이를 분석하고 수정함으로써 모델의 이해도를 높이고, 더 나은 결과를 얻을 수 있는 방법을 찾을 수 있습니다.\n",
      "\n",
      "### 5. 지속적인 개선\n",
      "프롬프트 엔지니어링은 반복적인 과정입니다. 사용자는 모델의 응답을 평가하고, 이를 바탕으로 프롬프트를 지속적으로 개선할 수 있습니다. 이러한 피드백 루프는 모델의 성능을 점진적으로 향상시키는 데 기여하며, 이는 인공지능 시스템의 발전에 중요한 요소입니다.\n",
      "\n",
      "### 결론\n",
      "프롬프트 엔지니어링은 인공지능 모델과의 효과적인 상호작용을 위한 핵심 요소로, 모델의 성능을 극대화하고 사용자 경험을 향상시키며, 다양한 응용 가능성을 열어줍니다. 따라서, 인공지능 기술을 활용하는 모든 분야에서 프롬프트 엔지니어링의 중요성을 인식하고 이를 적극적으로 활용하는 것이 필요합니다.\n",
      "\n",
      "검색된 실제 문서:\n",
      "1. 프롬프트 엔지니어링은 LLM에게 효과적인 지시를 제공하는 기술입니다.\n",
      "2. LangChain은 언어 모델을 활용한 애플리케이션 개발 프레임워크입니다.\n",
      "3. 벡터 데이터베이스는 임베딩을 저장하고 유사도 검색을 수행합니다.\n"
     ]
    }
   ],
   "source": [
    "# HyDE 프롬프트\n",
    "hyde_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"주어진 질문에 대한 가상의 답변을 작성하세요.\n",
    "    이 답변은 실제 문서처럼 작성되어야 하며, 구체적이고 상세해야 합니다.\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# HyDE 체인\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "def hyde_retrieval(question: str):\n",
    "    # 1. 가상 문서 생성\n",
    "    hypothetical_doc = hyde_chain.invoke({\"question\": question})\n",
    "    print(f\"생성된 가상 문서:\\n{hypothetical_doc}\\n\")\n",
    "    \n",
    "    # 2. 가상 문서로 검색\n",
    "    docs = vectorstore.similarity_search(hypothetical_doc, k=3)\n",
    "    return docs\n",
    "\n",
    "# 테스트\n",
    "question = \"프롬프트 엔지니어링의 중요성은?\"\n",
    "print(f\"질문: {question}\\n\")\n",
    "\n",
    "hyde_docs = hyde_retrieval(question)\n",
    "print(\"검색된 실제 문서:\")\n",
    "for i, doc in enumerate(hyde_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 의도 기반 라우팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "질문: LangChain이 뭔가요?\n",
      "--------------------------------------------------\n",
      "질문 유형: QueryType.DEFINITION\n",
      "이유: 질문은 LangChain의 정의나 개념을 묻고 있으므로 'definition' 카테고리에 해당합니다.\n",
      "\n",
      "답변: LangChain은 언어 모델을 활용하여 애플리케이션을 개발할 수 있도록 지원하는 프레임워크입니다. 이 프레임워크는 다양한 컴포넌트를 연결하여 복잡한 워크플로우를 구성할 수 있게 하여, 개발자들이 자연어 처리(NLP) 기능을 쉽게 통합하고 활용할 수 있도록 돕습니다. LangChain을 사용하면 언어 모델의 기능을 조합하여 다양한 애플리케이션을 효율적으로 구축할 수 있습니다.\n",
      "\n",
      "\n",
      "질문: RAG와 일반 LLM의 차이점은?\n",
      "--------------------------------------------------\n",
      "질문 유형: QueryType.COMPARISON\n",
      "이유: 질문은 RAG(리트리벌-어그멘테이션-제너레이션)와 일반 LLM(대형 언어 모델)의 차이점을 비교하고 있으므로, 비교를 요구하는 질문으로 분류됩니다.\n",
      "\n",
      "답변: RAG(검색 증강 생성)와 일반 LLM(대형 언어 모델)의 차이점은 주로 정보 처리 방식과 응답의 정확성에 있습니다. 아래에서 두 가지 접근 방식을 비교해 보겠습니다.\n",
      "\n",
      "### 1. 정보 출처\n",
      "- **RAG**: RAG는 외부 데이터베이스나 검색 엔진을 활용하여 관련 정보를 검색한 후, 이를 바탕으로 응답을 생성합니다. 이 과정에서 최신 정보나 특정 도메인에 대한 전문 지식을 포함할 수 있어, 보다 정확하고 신뢰할 수 있는 답변을 제공합니다.\n",
      "- **일반 LLM**: 일반 LLM은 훈련 데이터에 기반하여 응답을 생성합니다. 이 데이터는 고정되어 있으며, 모델이 훈련된 시점 이후의 정보나 특정 세부사항에 대한 접근이 제한적입니다. 따라서 최신 정보나 특정 질문에 대한 정확한 답변을 제공하기 어려울 수 있습니다.\n",
      "\n",
      "### 2. 응답의 정확성\n",
      "- **RAG**: RAG는 외부 지식을 활용하여 응답을 생성하기 때문에, 정보의 정확성과 관련성을 높일 수 있습니다. 사용자가 요청한 정보에 대해 보다 구체적이고 신뢰할 수 있는 답변을 제공할 수 있습니다.\n",
      "- **일반 LLM**: 일반 LLM은 훈련된 데이터에 기반하여 응답을 생성하므로, 때때로 부정확하거나 구식의 정보를 제공할 수 있습니다. 특히, 특정한 사실이나 데이터에 대한 질문에 대해 정확한 답변을 보장하기 어렵습니다.\n",
      "\n",
      "### 3. 유연성 및 적응성\n",
      "- **RAG**: RAG는 외부 데이터 소스를 활용하기 때문에, 새로운 정보나 변화하는 상황에 더 잘 적응할 수 있습니다. 사용자가 요구하는 정보에 따라 실시간으로 검색하고 응답을 조정할 수 있습니다.\n",
      "- **일반 LLM**: 일반 LLM은 훈련된 데이터에 고정되어 있어, 새로운 정보나 변화에 대한 적응력이 떨어집니다. 사용자가 요구하는 정보가 훈련 데이터에 포함되어 있지 않다면, 적절한 응답을 제공하기 어려울 수 있습니다.\n",
      "\n",
      "### 4. 사용 사례\n",
      "- **RAG**: RAG는 정보 검색이 중요한 상황, 예를 들어 고객 지원, 연구, 데이터 분석 등에서 유용합니다. 최신 정보나 특정 도메인 지식이 필요한 경우에 특히 효과적입니다.\n",
      "- **일반 LLM**: 일반 LLM은 창의적인 글쓰기, 대화형 AI, 기본적인 질문 응답 등에서 유용합니다. 특정한 정보의 정확성이 덜 중요한 경우에 적합합니다.\n",
      "\n",
      "결론적으로, RAG는 외부 지식을 활용하여 응답의 정확성을 높이는 반면, 일반 LLM은 훈련된 데이터에 기반하여 응답을 생성합니다. 이로 인해 RAG는 정보의 최신성과 정확성을 요구하는 상황에서 더 효과적일 수 있습니다.\n",
      "\n",
      "\n",
      "질문: 벡터 데이터베이스를 어떻게 사용하나요?\n",
      "--------------------------------------------------\n",
      "질문 유형: QueryType.HOWTO\n",
      "이유: 질문은 벡터 데이터베이스의 사용 방법에 대한 절차나 방법을 묻고 있으므로 'howto'로 분류됩니다.\n",
      "\n",
      "답변: 벡터 데이터베이스를 사용하여 임베딩을 저장하고 유사도 검색을 수행하는 과정은 다음과 같은 단계로 나눌 수 있습니다. 이 과정은 LangChain과 같은 프레임워크를 활용하여 언어 모델 기반 애플리케이션을 개발하는 데 유용합니다.\n",
      "\n",
      "### 1. 데이터 준비\n",
      "- **데이터 수집**: 검색하고자 하는 데이터(텍스트, 이미지 등)를 수집합니다.\n",
      "- **전처리**: 수집한 데이터를 정제하고 필요한 형식으로 변환합니다. 예를 들어, 텍스트 데이터를 소문자로 변환하거나 불용어를 제거할 수 있습니다.\n",
      "\n",
      "### 2. 임베딩 생성\n",
      "- **언어 모델 선택**: LangChain에서 사용할 언어 모델을 선택합니다. 예를 들어, OpenAI의 GPT 모델이나 다른 사전 훈련된 모델을 사용할 수 있습니다.\n",
      "- **임베딩 생성**: 선택한 언어 모델을 사용하여 데이터를 임베딩으로 변환합니다. 이 과정에서 각 데이터 포인트는 고차원 벡터로 표현됩니다.\n",
      "\n",
      "### 3. 벡터 데이터베이스 설정\n",
      "- **데이터베이스 선택**: Pinecone, Weaviate, FAISS 등과 같은 벡터 데이터베이스를 선택합니다.\n",
      "- **데이터베이스 초기화**: 선택한 벡터 데이터베이스를 초기화하고 필요한 설정을 구성합니다.\n",
      "\n",
      "### 4. 임베딩 저장\n",
      "- **임베딩 저장**: 생성한 임베딩을 벡터 데이터베이스에 저장합니다. 이때 각 임베딩에 대한 메타데이터(예: 원본 데이터의 ID, 텍스트 내용 등)를 함께 저장할 수 있습니다.\n",
      "\n",
      "### 5. 유사도 검색 수행\n",
      "- **쿼리 임베딩 생성**: 사용자가 입력한 쿼리를 임베딩으로 변환합니다.\n",
      "- **유사도 검색**: 쿼리 임베딩을 사용하여 벡터 데이터베이스에서 유사한 임베딩을 검색합니다. 이 과정에서 코사인 유사도, 유클리드 거리 등의 방법을 사용할 수 있습니다.\n",
      "- **결과 반환**: 검색된 유사한 데이터 포인트를 사용자에게 반환합니다.\n",
      "\n",
      "### 6. 결과 후처리\n",
      "- **결과 정렬 및 필터링**: 검색 결과를 정렬하고 필요에 따라 필터링하여 최종 결과를 개선합니다.\n",
      "- **결과 표시**: 사용자에게 결과를 적절한 형식으로 표시합니다. 예를 들어, 관련 텍스트, 이미지 또는 링크를 제공할 수 있습니다.\n",
      "\n",
      "### 7. 성능 모니터링 및 개선\n",
      "- **성능 평가**: 검색 결과의 품질을 평가하고, 필요에 따라 임베딩 생성 방법이나 데이터베이스 설정을 조정합니다.\n",
      "- **모델 업데이트**: 새로운 데이터를 추가하거나 모델을 재훈련하여 성능을 지속적으로 개선합니다.\n",
      "\n",
      "이러한 단계를 통해 벡터 데이터베이스를 효과적으로 활용하여 유사도 검색을 수행할 수 있습니다. LangChain과 같은 프레임워크를 사용하면 이러한 과정이 더욱 간편해지고, 다양한 언어 모델과의 통합이 용이해집니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class QueryType(str, Enum):\n",
    "    DEFINITION = \"definition\"\n",
    "    COMPARISON = \"comparison\"\n",
    "    HOWTO = \"howto\"\n",
    "    GENERAL = \"general\"\n",
    "\n",
    "class QueryClassification(BaseModel):\n",
    "    query_type: QueryType = Field(description=\"질문의 유형\")\n",
    "    reasoning: str = Field(description=\"분류 이유\")\n",
    "\n",
    "# 질문 분류 프롬프트\n",
    "classify_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"질문을 다음 카테고리 중 하나로 분류하세요:\n",
    "    - definition: 정의나 개념을 묻는 질문\n",
    "    - comparison: 비교를 요구하는 질문\n",
    "    - howto: 방법이나 절차를 묻는 질문\n",
    "    - general: 일반적인 질문\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 분류기\n",
    "classifier = llm.with_structured_output(QueryClassification)\n",
    "classify_chain = classify_prompt | classifier\n",
    "\n",
    "# 각 유형별 프롬프트\n",
    "prompts = {\n",
    "    QueryType.DEFINITION: \"정의: {context}를 바탕으로 '{question}'을(를) 명확하게 정의하세요.\",\n",
    "    QueryType.COMPARISON: \"비교: {context}를 바탕으로 '{question}'에 대해 비교 설명하세요.\",\n",
    "    QueryType.HOWTO: \"방법: {context}를 바탕으로 '{question}'을(를) 수행하는 단계를 설명하세요.\",\n",
    "    QueryType.GENERAL: \"일반: {context}를 바탕으로 '{question}'에 답하세요.\"\n",
    "}\n",
    "\n",
    "def routed_rag(question: str):\n",
    "    # 1. 질문 분류\n",
    "    classification = classify_chain.invoke({\"question\": question})\n",
    "    print(f\"질문 유형: {classification.query_type}\")\n",
    "    print(f\"이유: {classification.reasoning}\\n\")\n",
    "    \n",
    "    # 2. 문서 검색\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # 3. 유형별 프롬프트 선택\n",
    "    prompt_template = prompts[classification.query_type]\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"당신은 도움이 되는 AI 어시스턴트입니다.\"),\n",
    "        (\"human\", prompt_template)\n",
    "    ])\n",
    "    \n",
    "    # 4. 답변 생성\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    return answer\n",
    "\n",
    "# 테스트\n",
    "test_questions = [\n",
    "    \"LangChain이 뭔가요?\",\n",
    "    \"RAG와 일반 LLM의 차이점은?\",\n",
    "    \"벡터 데이터베이스를 어떻게 사용하나요?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\n질문: {q}\")\n",
    "    print(\"-\" * 50)\n",
    "    answer = routed_rag(q)\n",
    "    print(f\"답변: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Self-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 2000년 이전에 개발된 프로그래밍 언어\n",
      "파싱된 쿼리: query='프로그래밍 언어' category='programming' year_min=None year_max=2000\n",
      "\n",
      "검색 결과:\n",
      "- Python은 1991년에 귀도 반 로섬이 개발한 프로그래밍 언어입니다.\n",
      "  메타데이터: {'year': 1991, 'category': 'programming', 'language': 'Python'}\n",
      "- JavaScript는 1995년에 브렌던 아이크가 개발한 스크립트 언어입니다.\n",
      "  메타데이터: {'year': 1995, 'category': 'programming', 'language': 'JavaScript'}\n",
      "\n",
      "============================================================\n",
      "\n",
      "질문: AI 분야의 최신 기술\n",
      "파싱된 쿼리: query='최신 AI 기술' category='ai' year_min=2023 year_max=2023\n",
      "\n",
      "검색 결과:\n",
      "\n",
      "============================================================\n",
      "\n",
      "질문: 프로그래밍 언어의 역사\n",
      "파싱된 쿼리: query='프로그래밍 언어의 역사' category='programming' year_min=None year_max=None\n",
      "\n",
      "검색 결과:\n",
      "- Python은 1991년에 귀도 반 로섬이 개발한 프로그래밍 언어입니다.\n",
      "  메타데이터: {'year': 1991, 'category': 'programming', 'language': 'Python'}\n",
      "- JavaScript는 1995년에 브렌던 아이크가 개발한 스크립트 언어입니다.\n",
      "  메타데이터: {'year': 1995, 'category': 'programming', 'language': 'JavaScript'}\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import Optional\n",
    "\n",
    "# 메타데이터가 있는 문서 생성\n",
    "docs_with_metadata = [\n",
    "    Document(\n",
    "        page_content=\"Python은 1991년에 귀도 반 로섬이 개발한 프로그래밍 언어입니다.\",\n",
    "        metadata={\"year\": 1991, \"category\": \"programming\", \"language\": \"Python\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"JavaScript는 1995년에 브렌던 아이크가 개발한 스크립트 언어입니다.\",\n",
    "        metadata={\"year\": 1995, \"category\": \"programming\", \"language\": \"JavaScript\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"머신러닝은 2010년대에 딥러닝과 함께 급속히 발전했습니다.\",\n",
    "        metadata={\"year\": 2010, \"category\": \"ai\", \"topic\": \"machine learning\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"트랜스포머 모델은 2017년에 구글이 발표한 혁신적인 아키텍처입니다.\",\n",
    "        metadata={\"year\": 2017, \"category\": \"ai\", \"topic\": \"transformer\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 벡터 저장소 생성\n",
    "metadata_vectorstore = FAISS.from_documents(docs_with_metadata, embeddings)\n",
    "\n",
    "# Self-Query 파싱\n",
    "class SelfQuery(BaseModel):\n",
    "    query: str = Field(description=\"검색할 내용\")\n",
    "    category: Optional[str] = Field(default=None, description=\"카테고리 필터\")\n",
    "    year_min: Optional[int] = Field(default=None, description=\"최소 연도\")\n",
    "    year_max: Optional[int] = Field(default=None, description=\"최대 연도\")\n",
    "\n",
    "# Self-Query 프롬프트\n",
    "self_query_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"사용자의 질문을 분석하여 검색 쿼리와 필터를 추출하세요.\n",
    "    카테고리: programming, ai\n",
    "    연도 범위도 추출할 수 있습니다.\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "self_query_llm = llm.with_structured_output(SelfQuery)\n",
    "self_query_chain = self_query_prompt | self_query_llm\n",
    "\n",
    "def self_query_search(question: str):\n",
    "    # 쿼리 파싱\n",
    "    parsed = self_query_chain.invoke({\"question\": question})\n",
    "    print(f\"파싱된 쿼리: {parsed}\\n\")\n",
    "    \n",
    "    # 검색 수행\n",
    "    results = metadata_vectorstore.similarity_search(parsed.query, k=5)\n",
    "    \n",
    "    # 메타데이터 필터링\n",
    "    filtered = []\n",
    "    for doc in results:\n",
    "        if parsed.category and doc.metadata.get(\"category\") != parsed.category:\n",
    "            continue\n",
    "        if parsed.year_min and doc.metadata.get(\"year\", 0) < parsed.year_min:\n",
    "            continue\n",
    "        if parsed.year_max and doc.metadata.get(\"year\", float('inf')) > parsed.year_max:\n",
    "            continue\n",
    "        filtered.append(doc)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# 테스트\n",
    "test_queries = [\n",
    "    \"2000년 이전에 개발된 프로그래밍 언어\",\n",
    "    \"AI 분야의 최신 기술\",\n",
    "    \"프로그래밍 언어의 역사\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"질문: {q}\")\n",
    "    results = self_query_search(q)\n",
    "    print(\"검색 결과:\")\n",
    "    for doc in results:\n",
    "        print(f\"- {doc.page_content}\")\n",
    "        print(f\"  메타데이터: {doc.metadata}\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SQL 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 가장 비싼 제품은?\n",
      "생성된 SQL: SELECT * FROM products ORDER BY price DESC LIMIT 1;\n",
      "\n",
      "결과:\n",
      "   id         name category    price  stock\n",
      "0   1  MacBook Pro   Laptop  2499.99     10\n",
      "\n",
      "============================================================\n",
      "\n",
      "질문: 재고가 20개 이상인 제품들\n",
      "생성된 SQL: SELECT * FROM products WHERE stock >= 20;\n",
      "\n",
      "결과:\n",
      "   id         name  category   price  stock\n",
      "0   2    iPhone 15     Phone  999.99     25\n",
      "1   3  AirPods Pro     Audio  249.99     50\n",
      "2   5  Apple Watch  Wearable  399.99     30\n",
      "\n",
      "============================================================\n",
      "\n",
      "질문: 카테고리별 평균 가격\n",
      "생성된 SQL: SELECT category, AVG(price) AS average_price\n",
      "FROM products\n",
      "GROUP BY category;\n",
      "\n",
      "결과:\n",
      "   category  average_price\n",
      "0     Audio         249.99\n",
      "1    Laptop        2499.99\n",
      "2     Phone         999.99\n",
      "3    Tablet         599.99\n",
      "4  Wearable         399.99\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# SQLite 데이터베이스 생성\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 테이블 생성\n",
    "cursor.execute('''\n",
    "    CREATE TABLE products (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        category TEXT,\n",
    "        price REAL,\n",
    "        stock INTEGER\n",
    "    )\n",
    "''')\n",
    "\n",
    "# 샘플 데이터 삽입\n",
    "products = [\n",
    "    (1, 'MacBook Pro', 'Laptop', 2499.99, 10),\n",
    "    (2, 'iPhone 15', 'Phone', 999.99, 25),\n",
    "    (3, 'AirPods Pro', 'Audio', 249.99, 50),\n",
    "    (4, 'iPad Air', 'Tablet', 599.99, 15),\n",
    "    (5, 'Apple Watch', 'Wearable', 399.99, 30)\n",
    "]\n",
    "\n",
    "cursor.executemany('INSERT INTO products VALUES (?, ?, ?, ?, ?)', products)\n",
    "conn.commit()\n",
    "\n",
    "# SQL 생성 프롬프트\n",
    "sql_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"다음 테이블 스키마를 참고하여 자연어 질문을 SQL 쿼리로 변환하세요:\n",
    "    \n",
    "    products 테이블:\n",
    "    - id: INTEGER (PRIMARY KEY)\n",
    "    - name: TEXT\n",
    "    - category: TEXT\n",
    "    - price: REAL\n",
    "    - stock: INTEGER\n",
    "    \n",
    "    SQL 쿼리만 반환하고 다른 설명은 포함하지 마세요.\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "sql_chain = sql_prompt | llm | StrOutputParser()\n",
    "\n",
    "def execute_sql_query(question: str):\n",
    "    # SQL 생성\n",
    "    sql_query = sql_chain.invoke({\"question\": question})\n",
    "    sql_query = sql_query.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "    print(f\"생성된 SQL: {sql_query}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # SQL 실행\n",
    "        result = pd.read_sql_query(sql_query, conn)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"에러: {e}\"\n",
    "\n",
    "# 테스트\n",
    "questions = [\n",
    "    \"가장 비싼 제품은?\",\n",
    "    \"재고가 20개 이상인 제품들\",\n",
    "    \"카테고리별 평균 가격\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"질문: {q}\")\n",
    "    result = execute_sql_query(q)\n",
    "    print(\"결과:\")\n",
    "    print(result)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 과제\n",
    "\n",
    "1. 여러 RAG 기법을 조합한 하이브리드 시스템 구현\n",
    "2. 평가 메트릭을 추가하여 각 기법의 성능 비교\n",
    "3. 실제 문서를 사용한 도메인 특화 RAG 시스템 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하세요\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
