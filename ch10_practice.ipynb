{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: LangGraph Evaluation ì‹¤ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ LangGraph ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í‰ê°€ì™€ ì„±ëŠ¥ ì¸¡ì •ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# í‰ê°€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "@dataclass\n",
    "class EvaluationCase:\n",
    "    \"\"\"í‰ê°€ ì¼€ì´ìŠ¤\"\"\"\n",
    "    id: str\n",
    "    input: Dict[str, Any]\n",
    "    expected_output: Dict[str, Any]\n",
    "    metadata: Dict[str, Any]\n",
    "    tags: List[str]\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "\n",
    "class EvaluationDataset:\n",
    "    \"\"\"í‰ê°€ ë°ì´í„°ì…‹ ê´€ë¦¬\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str = \"\"):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.cases: List[EvaluationCase] = []\n",
    "        self.created_at = datetime.now()\n",
    "        self.version = \"1.0.0\"\n",
    "    \n",
    "    def add_case(self, \n",
    "                 input_data: Dict,\n",
    "                 expected_output: Dict,\n",
    "                 metadata: Dict = None,\n",
    "                 tags: List[str] = None):\n",
    "        \"\"\"í‰ê°€ ì¼€ì´ìŠ¤ ì¶”ê°€\"\"\"\n",
    "        case = EvaluationCase(\n",
    "            id=str(uuid.uuid4()),\n",
    "            input=input_data,\n",
    "            expected_output=expected_output,\n",
    "            metadata=metadata or {},\n",
    "            tags=tags or []\n",
    "        )\n",
    "        self.cases.append(case)\n",
    "    \n",
    "    def save_to_file(self, filepath: str):\n",
    "        \"\"\"íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        data = {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description,\n",
    "            \"version\": self.version,\n",
    "            \"created_at\": self.created_at.isoformat(),\n",
    "            \"cases\": [case.to_dict() for case in self.cases]\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"ë°ì´í„°ì…‹ì´ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_file(cls, filepath: str):\n",
    "        \"\"\"íŒŒì¼ì—ì„œ ë¡œë“œ\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        dataset = cls(data[\"name\"], data.get(\"description\", \"\"))\n",
    "        dataset.version = data.get(\"version\", \"1.0.0\")\n",
    "        \n",
    "        for case_data in data[\"cases\"]:\n",
    "            case = EvaluationCase(**case_data)\n",
    "            dataset.cases.append(case)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"DataFrameìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        return pd.DataFrame([case.to_dict() for case in self.cases])\n",
    "    \n",
    "    def filter_by_tags(self, tags: List[str]) -> List[EvaluationCase]:\n",
    "        \"\"\"íƒœê·¸ë¡œ í•„í„°ë§\"\"\"\n",
    "        filtered = []\n",
    "        for case in self.cases:\n",
    "            if any(tag in case.tags for tag in tags):\n",
    "                filtered.append(case)\n",
    "        return filtered\n",
    "\n",
    "# RAG í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±\n",
    "def create_rag_dataset() -> EvaluationDataset:\n",
    "    \"\"\"RAG ì‹œìŠ¤í…œ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    dataset = EvaluationDataset(\n",
    "        name=\"RAG Evaluation Dataset\",\n",
    "        description=\"RAG ì‹œìŠ¤í…œ í‰ê°€ë¥¼ ìœ„í•œ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°ì…‹\"\n",
    "    )\n",
    "    \n",
    "    # í‰ê°€ ì¼€ì´ìŠ¤ë“¤\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"input\": {\"question\": \"LangChainì´ ë¬´ì—‡ì¸ê°€ìš”?\"},\n",
    "            \"expected\": {\n",
    "                \"relevance\": \"high\",\n",
    "                \"keywords\": [\"í”„ë ˆì„ì›Œí¬\", \"LLM\", \"ì• í”Œë¦¬ì¼€ì´ì…˜\"]\n",
    "            },\n",
    "            \"tags\": [\"definition\", \"basic\"]\n",
    "        },\n",
    "        {\n",
    "            \"input\": {\"question\": \"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ ì¥ì ì€?\"},\n",
    "            \"expected\": {\n",
    "                \"relevance\": \"high\",\n",
    "                \"keywords\": [\"ìœ ì‚¬ë„\", \"ê²€ìƒ‰\", \"ì„ë² ë”©\"]\n",
    "            },\n",
    "            \"tags\": [\"technical\", \"database\"]\n",
    "        },\n",
    "        {\n",
    "            \"input\": {\"question\": \"RAGì™€ íŒŒì¸íŠœë‹ì˜ ì°¨ì´ì ì€?\"},\n",
    "            \"expected\": {\n",
    "                \"relevance\": \"high\",\n",
    "                \"keywords\": [\"ê²€ìƒ‰\", \"í•™ìŠµ\", \"ë¹„ìš©\", \"ìœ ì—°ì„±\"]\n",
    "            },\n",
    "            \"tags\": [\"comparison\", \"advanced\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        dataset.add_case(\n",
    "            input_data=case[\"input\"],\n",
    "            expected_output=case[\"expected\"],\n",
    "            tags=case[\"tags\"]\n",
    "        )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# SQL í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±\n",
    "def create_sql_dataset() -> EvaluationDataset:\n",
    "    \"\"\"SQL ìƒì„± í‰ê°€ ë°ì´í„°ì…‹\"\"\"\n",
    "    dataset = EvaluationDataset(\n",
    "        name=\"SQL Generation Dataset\",\n",
    "        description=\"SQL ì¿¼ë¦¬ ìƒì„± í‰ê°€ ë°ì´í„°ì…‹\"\n",
    "    )\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"input\": {\"question\": \"ëª¨ë“  ì§ì›ì˜ ì´ë¦„ê³¼ ë¶€ì„œë¥¼ ì¡°íšŒí•˜ì„¸ìš”\"},\n",
    "            \"expected\": {\n",
    "                \"query_type\": \"SELECT\",\n",
    "                \"tables\": [\"employees\"],\n",
    "                \"columns\": [\"name\", \"department\"]\n",
    "            },\n",
    "            \"tags\": [\"basic\", \"select\"]\n",
    "        },\n",
    "        {\n",
    "            \"input\": {\"question\": \"ë¶€ì„œë³„ í‰ê·  ê¸‰ì—¬ë¥¼ ê³„ì‚°í•˜ì„¸ìš”\"},\n",
    "            \"expected\": {\n",
    "                \"query_type\": \"SELECT\",\n",
    "                \"functions\": [\"AVG\"],\n",
    "                \"group_by\": True\n",
    "            },\n",
    "            \"tags\": [\"aggregation\", \"group_by\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        dataset.add_case(\n",
    "            input_data=case[\"input\"],\n",
    "            expected_output=case[\"expected\"],\n",
    "            tags=case[\"tags\"]\n",
    "        )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„± ë° ì €ì¥\n",
    "rag_dataset = create_rag_dataset()\n",
    "sql_dataset = create_sql_dataset()\n",
    "\n",
    "print(\"ğŸ“Š RAG ë°ì´í„°ì…‹:\")\n",
    "print(f\"ì¼€ì´ìŠ¤ ìˆ˜: {len(rag_dataset.cases)}\")\n",
    "print(rag_dataset.to_dataframe()[[\"id\", \"tags\"]].head())\n",
    "\n",
    "print(\"\\nğŸ“Š SQL ë°ì´í„°ì…‹:\")\n",
    "print(f\"ì¼€ì´ìŠ¤ ìˆ˜: {len(sql_dataset.cases)}\")\n",
    "print(sql_dataset.to_dataframe()[[\"id\", \"tags\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# í‰ê°€ ë©”íŠ¸ë¦­ í´ë˜ìŠ¤\n",
    "class EvaluationMetrics:\n",
    "    \"\"\"í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_accuracy(predictions: List, ground_truth: List) -> float:\n",
    "        \"\"\"ì •í™•ë„ ê³„ì‚°\"\"\"\n",
    "        if len(predictions) != len(ground_truth):\n",
    "            raise ValueError(\"ì˜ˆì¸¡ê³¼ ì •ë‹µì˜ ê¸¸ì´ê°€ ë‹¤ë¦…ë‹ˆë‹¤.\")\n",
    "        \n",
    "        correct = sum(1 for p, g in zip(predictions, ground_truth) if p == g)\n",
    "        return correct / len(predictions)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_bleu_score(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"BLEU ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)\"\"\"\n",
    "        from nltk.translate.bleu_score import sentence_bleu\n",
    "        reference_tokens = reference.split()\n",
    "        hypothesis_tokens = hypothesis.split()\n",
    "        return sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rouge_score(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "        \"\"\"ROUGE ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "        # ê°„ë‹¨í•œ ROUGE-1 êµ¬í˜„\n",
    "        ref_tokens = set(reference.lower().split())\n",
    "        hyp_tokens = set(hypothesis.lower().split())\n",
    "        \n",
    "        if not hyp_tokens:\n",
    "            return {\"precision\": 0, \"recall\": 0, \"f1\": 0}\n",
    "        \n",
    "        overlap = ref_tokens.intersection(hyp_tokens)\n",
    "        precision = len(overlap) / len(hyp_tokens) if hyp_tokens else 0\n",
    "        recall = len(overlap) / len(ref_tokens) if ref_tokens else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_semantic_similarity(text1: str, text2: str, embeddings) -> float:\n",
    "        \"\"\"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "        emb1 = embeddings.embed_query(text1)\n",
    "        emb2 = embeddings.embed_query(text2)\n",
    "        \n",
    "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
    "        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "        return float(similarity)\n",
    "\n",
    "# LLM ê¸°ë°˜ í‰ê°€ì\n",
    "class LLMEvaluator:\n",
    "    \"\"\"LLMì„ ì‚¬ìš©í•œ í‰ê°€\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=0)\n",
    "    \n",
    "    def evaluate_relevance(self, question: str, answer: str) -> Dict[str, Any]:\n",
    "        \"\"\"ê´€ë ¨ì„± í‰ê°€\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        ì§ˆë¬¸: {question}\n",
    "        ë‹µë³€: {answer}\n",
    "        \n",
    "        ìœ„ ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.\n",
    "        ì ìˆ˜: 1-10 (10ì´ ê°€ì¥ ê´€ë ¨ì„± ë†’ìŒ)\n",
    "        ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”.\n",
    "        \n",
    "        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ: {{\"score\": <ì ìˆ˜>, \"reason\": \"<ì´ìœ >\"}}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.content)\n",
    "            return result\n",
    "        except:\n",
    "            return {\"score\": 0, \"reason\": \"í‰ê°€ ì‹¤íŒ¨\"}\n",
    "    \n",
    "    def evaluate_correctness(self, answer: str, expected: str) -> Dict[str, Any]:\n",
    "        \"\"\"ì •í™•ì„± í‰ê°€\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        ìƒì„±ëœ ë‹µë³€: {answer}\n",
    "        ì˜ˆìƒ ë‹µë³€: {expected}\n",
    "        \n",
    "        ìƒì„±ëœ ë‹µë³€ì´ ì˜ˆìƒ ë‹µë³€ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.\n",
    "        ì˜ë¯¸ì ìœ¼ë¡œ ê°™ì€ ë‚´ìš©ì´ë©´ ë†’ì€ ì ìˆ˜ë¥¼ ì£¼ì„¸ìš”.\n",
    "        \n",
    "        ì ìˆ˜: 1-10\n",
    "        JSON í˜•ì‹: {{\"score\": <ì ìˆ˜>, \"differences\": [\"<ì°¨ì´ì ë“¤>\"]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.content)\n",
    "            return result\n",
    "        except:\n",
    "            return {\"score\": 0, \"differences\": [\"í‰ê°€ ì‹¤íŒ¨\"]}\n",
    "    \n",
    "    def evaluate_hallucination(self, answer: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"í™˜ê° í‰ê°€\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        ì»¨í…ìŠ¤íŠ¸: {context}\n",
    "        ìƒì„±ëœ ë‹µë³€: {answer}\n",
    "        \n",
    "        ë‹µë³€ì— ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´(í™˜ê°)ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n",
    "        \n",
    "        JSON í˜•ì‹: {{\n",
    "            \"has_hallucination\": <true/false>,\n",
    "            \"hallucinated_parts\": [\"<í™˜ê° ë¶€ë¶„ë“¤>\"],\n",
    "            \"confidence\": <0-1>\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.content)\n",
    "            return result\n",
    "        except:\n",
    "            return {\n",
    "                \"has_hallucination\": False,\n",
    "                \"hallucinated_parts\": [],\n",
    "                \"confidence\": 0\n",
    "            }\n",
    "\n",
    "# ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸\n",
    "metrics = EvaluationMetrics()\n",
    "llm_evaluator = LLMEvaluator()\n",
    "\n",
    "# ì˜ˆì œ í…ìŠ¤íŠ¸\n",
    "reference = \"LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\"\n",
    "hypothesis = \"LangChainì€ LLM ê¸°ë°˜ ì•± ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "# ROUGE ì ìˆ˜\n",
    "rouge_scores = metrics.calculate_rouge_score(reference, hypothesis)\n",
    "print(\"ğŸ“Š ROUGE ì ìˆ˜:\")\n",
    "print(f\"Precision: {rouge_scores['precision']:.3f}\")\n",
    "print(f\"Recall: {rouge_scores['recall']:.3f}\")\n",
    "print(f\"F1: {rouge_scores['f1']:.3f}\")\n",
    "\n",
    "# LLM í‰ê°€\n",
    "print(\"\\nğŸ¤– LLM ê¸°ë°˜ í‰ê°€:\")\n",
    "relevance = llm_evaluator.evaluate_relevance(\n",
    "    \"LangChainì´ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    hypothesis\n",
    ")\n",
    "print(f\"ê´€ë ¨ì„± ì ìˆ˜: {relevance.get('score', 0)}/10\")\n",
    "print(f\"ì´ìœ : {relevance.get('reason', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langchain_core.messages import HumanMessage\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "# ê·¸ë˜í”„ í‰ê°€ í”„ë ˆì„ì›Œí¬\n",
    "class GraphEvaluator:\n",
    "    \"\"\"LangGraph í‰ê°€ í”„ë ˆì„ì›Œí¬\"\"\"\n",
    "    \n",
    "    def __init__(self, graph, dataset: EvaluationDataset):\n",
    "        self.graph = graph\n",
    "        self.dataset = dataset\n",
    "        self.results = []\n",
    "        self.metrics = EvaluationMetrics()\n",
    "        self.llm_evaluator = LLMEvaluator()\n",
    "    \n",
    "    def evaluate(self, \n",
    "                 eval_functions: List[Callable] = None,\n",
    "                 verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"ì „ì²´ í‰ê°€ ì‹¤í–‰\"\"\"\n",
    "        \n",
    "        total_cases = len(self.dataset.cases)\n",
    "        print(f\"ğŸ“Š í‰ê°€ ì‹œì‘: {total_cases}ê°œ ì¼€ì´ìŠ¤\")\n",
    "        \n",
    "        for i, case in enumerate(self.dataset.cases, 1):\n",
    "            if verbose:\n",
    "                print(f\"\\ní‰ê°€ ì¤‘ [{i}/{total_cases}]: {case.id}\")\n",
    "            \n",
    "            # ê·¸ë˜í”„ ì‹¤í–‰\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                output = self.graph.invoke(case.input)\n",
    "                latency = time.time() - start_time\n",
    "                success = True\n",
    "                error = None\n",
    "            except Exception as e:\n",
    "                output = None\n",
    "                latency = time.time() - start_time\n",
    "                success = False\n",
    "                error = str(e)\n",
    "            \n",
    "            # í‰ê°€ ì‹¤í–‰\n",
    "            eval_results = {\n",
    "                \"case_id\": case.id,\n",
    "                \"success\": success,\n",
    "                \"latency\": latency,\n",
    "                \"error\": error\n",
    "            }\n",
    "            \n",
    "            if success and eval_functions:\n",
    "                for eval_func in eval_functions:\n",
    "                    metric_name = eval_func.__name__\n",
    "                    metric_value = eval_func(output, case.expected_output)\n",
    "                    eval_results[metric_name] = metric_value\n",
    "            \n",
    "            self.results.append(eval_results)\n",
    "        \n",
    "        # ì§‘ê³„\n",
    "        summary = self._calculate_summary()\n",
    "        \n",
    "        if verbose:\n",
    "            self._print_summary(summary)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _calculate_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"í‰ê°€ ê²°ê³¼ ìš”ì•½\"\"\"\n",
    "        successful_results = [r for r in self.results if r[\"success\"]]\n",
    "        \n",
    "        summary = {\n",
    "            \"total_cases\": len(self.results),\n",
    "            \"successful_cases\": len(successful_results),\n",
    "            \"failed_cases\": len(self.results) - len(successful_results),\n",
    "            \"success_rate\": len(successful_results) / len(self.results) if self.results else 0,\n",
    "            \"avg_latency\": np.mean([r[\"latency\"] for r in self.results]) if self.results else 0,\n",
    "            \"p95_latency\": np.percentile([r[\"latency\"] for r in self.results], 95) if self.results else 0\n",
    "        }\n",
    "        \n",
    "        # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì§‘ê³„\n",
    "        metric_names = set()\n",
    "        for result in successful_results:\n",
    "            metric_names.update(k for k in result.keys() \n",
    "                              if k not in [\"case_id\", \"success\", \"latency\", \"error\"])\n",
    "        \n",
    "        for metric in metric_names:\n",
    "            values = [r.get(metric, 0) for r in successful_results if metric in r]\n",
    "            if values:\n",
    "                summary[f\"avg_{metric}\"] = np.mean(values)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _print_summary(self, summary: Dict[str, Any]):\n",
    "        \"\"\"ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“ˆ í‰ê°€ ê²°ê³¼ ìš”ì•½\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"ì´ ì¼€ì´ìŠ¤: {summary['total_cases']}\")\n",
    "        print(f\"ì„±ê³µ: {summary['successful_cases']} ({summary['success_rate']:.1%})\")\n",
    "        print(f\"ì‹¤íŒ¨: {summary['failed_cases']}\")\n",
    "        print(f\"\\ní‰ê·  ì§€ì—°ì‹œê°„: {summary['avg_latency']*1000:.2f}ms\")\n",
    "        print(f\"P95 ì§€ì—°ì‹œê°„: {summary['p95_latency']*1000:.2f}ms\")\n",
    "        \n",
    "        # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­\n",
    "        custom_metrics = {k: v for k, v in summary.items() \n",
    "                         if k.startswith(\"avg_\") and not k.endswith(\"latency\")}\n",
    "        if custom_metrics:\n",
    "            print(\"\\nì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­:\")\n",
    "            for metric, value in custom_metrics.items():\n",
    "                print(f\"  {metric}: {value:.3f}\")\n",
    "    \n",
    "    def export_results(self, filepath: str):\n",
    "        \"\"\"ê²°ê³¼ ë‚´ë³´ë‚´ê¸°\"\"\"\n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"ê²°ê³¼ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ìƒ˜í”Œ ê·¸ë˜í”„ ìƒì„±\n",
    "def create_sample_graph():\n",
    "    \"\"\"í‰ê°€ìš© ìƒ˜í”Œ ê·¸ë˜í”„\"\"\"\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    \n",
    "    def process_node(state: MessagesState):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    workflow.add_node(\"process\", process_node)\n",
    "    workflow.set_entry_point(\"process\")\n",
    "    workflow.add_edge(\"process\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "def evaluate_response_length(output: Dict, expected: Dict) -> float:\n",
    "    \"\"\"ì‘ë‹µ ê¸¸ì´ í‰ê°€\"\"\"\n",
    "    if \"messages\" in output and output[\"messages\"]:\n",
    "        response_length = len(output[\"messages\"][-1].content)\n",
    "        return min(response_length / 100, 1.0)  # 100ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_keyword_presence(output: Dict, expected: Dict) -> float:\n",
    "    \"\"\"í‚¤ì›Œë“œ í¬í•¨ í‰ê°€\"\"\"\n",
    "    if \"keywords\" not in expected:\n",
    "        return 1.0\n",
    "    \n",
    "    if \"messages\" in output and output[\"messages\"]:\n",
    "        response = output[\"messages\"][-1].content.lower()\n",
    "        keywords = expected.get(\"keywords\", [])\n",
    "        \n",
    "        if keywords:\n",
    "            present = sum(1 for kw in keywords if kw.lower() in response)\n",
    "            return present / len(keywords)\n",
    "    return 0.0\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰\n",
    "print(\"ğŸš€ ê·¸ë˜í”„ í‰ê°€ ì‹œì‘\\n\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ë°ì´í„°ì…‹\n",
    "test_dataset = EvaluationDataset(\"Test Dataset\")\n",
    "test_dataset.add_case(\n",
    "    input_data={\"messages\": [HumanMessage(content=\"LangChainì´ ë­ì•¼?\")]},\n",
    "    expected_output={\"keywords\": [\"í”„ë ˆì„ì›Œí¬\", \"LLM\"]},\n",
    "    tags=[\"basic\"]\n",
    ")\n",
    "test_dataset.add_case(\n",
    "    input_data={\"messages\": [HumanMessage(content=\"Python ì„¤ëª…í•´ì¤˜\")]},\n",
    "    expected_output={\"keywords\": [\"í”„ë¡œê·¸ë˜ë°\", \"ì–¸ì–´\"]},\n",
    "    tags=[\"basic\"]\n",
    ")\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„± ë° í‰ê°€\n",
    "sample_graph = create_sample_graph()\n",
    "evaluator = GraphEvaluator(sample_graph, test_dataset)\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰\n",
    "summary = evaluator.evaluate(\n",
    "    eval_functions=[evaluate_response_length, evaluate_keyword_presence],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A/B Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "class ABTestFramework:\n",
    "    \"\"\"A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_a, graph_b, dataset: EvaluationDataset):\n",
    "        self.graph_a = graph_a\n",
    "        self.graph_b = graph_b\n",
    "        self.dataset = dataset\n",
    "        self.results_a = []\n",
    "        self.results_b = []\n",
    "    \n",
    "    def run_test(self, \n",
    "                 metric_func: Callable,\n",
    "                 sample_size: int = None,\n",
    "                 confidence_level: float = 0.95) -> Dict[str, Any]:\n",
    "        \"\"\"A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰\"\"\"\n",
    "        \n",
    "        # ìƒ˜í”Œ í¬ê¸° ê²°ì •\n",
    "        cases = self.dataset.cases[:sample_size] if sample_size else self.dataset.cases\n",
    "        \n",
    "        print(f\"ğŸ”¬ A/B í…ŒìŠ¤íŠ¸ ì‹œì‘ (n={len(cases)})\")\n",
    "        \n",
    "        for case in cases:\n",
    "            # ê·¸ë˜í”„ A í‰ê°€\n",
    "            try:\n",
    "                start = time.time()\n",
    "                output_a = self.graph_a.invoke(case.input)\n",
    "                latency_a = time.time() - start\n",
    "                metric_a = metric_func(output_a, case.expected_output)\n",
    "                self.results_a.append({\n",
    "                    \"metric\": metric_a,\n",
    "                    \"latency\": latency_a,\n",
    "                    \"success\": True\n",
    "                })\n",
    "            except Exception as e:\n",
    "                self.results_a.append({\n",
    "                    \"metric\": 0,\n",
    "                    \"latency\": 0,\n",
    "                    \"success\": False\n",
    "                })\n",
    "            \n",
    "            # ê·¸ë˜í”„ B í‰ê°€\n",
    "            try:\n",
    "                start = time.time()\n",
    "                output_b = self.graph_b.invoke(case.input)\n",
    "                latency_b = time.time() - start\n",
    "                metric_b = metric_func(output_b, case.expected_output)\n",
    "                self.results_b.append({\n",
    "                    \"metric\": metric_b,\n",
    "                    \"latency\": latency_b,\n",
    "                    \"success\": True\n",
    "                })\n",
    "            except Exception as e:\n",
    "                self.results_b.append({\n",
    "                    \"metric\": 0,\n",
    "                    \"latency\": 0,\n",
    "                    \"success\": False\n",
    "                })\n",
    "        \n",
    "        # í†µê³„ ë¶„ì„\n",
    "        analysis = self._analyze_results(confidence_level)\n",
    "        self._print_analysis(analysis)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_results(self, confidence_level: float) -> Dict[str, Any]:\n",
    "        \"\"\"ê²°ê³¼ ë¶„ì„\"\"\"\n",
    "        metrics_a = [r[\"metric\"] for r in self.results_a if r[\"success\"]]\n",
    "        metrics_b = [r[\"metric\"] for r in self.results_b if r[\"success\"]]\n",
    "        \n",
    "        latencies_a = [r[\"latency\"] for r in self.results_a if r[\"success\"]]\n",
    "        latencies_b = [r[\"latency\"] for r in self.results_b if r[\"success\"]]\n",
    "        \n",
    "        # t-test\n",
    "        if len(metrics_a) > 1 and len(metrics_b) > 1:\n",
    "            t_stat, p_value = stats.ttest_ind(metrics_a, metrics_b)\n",
    "        else:\n",
    "            t_stat, p_value = 0, 1\n",
    "        \n",
    "        return {\n",
    "            \"graph_a\": {\n",
    "                \"mean_metric\": np.mean(metrics_a) if metrics_a else 0,\n",
    "                \"std_metric\": np.std(metrics_a) if metrics_a else 0,\n",
    "                \"mean_latency\": np.mean(latencies_a) if latencies_a else 0,\n",
    "                \"success_rate\": sum(r[\"success\"] for r in self.results_a) / len(self.results_a)\n",
    "            },\n",
    "            \"graph_b\": {\n",
    "                \"mean_metric\": np.mean(metrics_b) if metrics_b else 0,\n",
    "                \"std_metric\": np.std(metrics_b) if metrics_b else 0,\n",
    "                \"mean_latency\": np.mean(latencies_b) if latencies_b else 0,\n",
    "                \"success_rate\": sum(r[\"success\"] for r in self.results_b) / len(self.results_b)\n",
    "            },\n",
    "            \"statistical_test\": {\n",
    "                \"t_statistic\": t_stat,\n",
    "                \"p_value\": p_value,\n",
    "                \"significant\": p_value < (1 - confidence_level),\n",
    "                \"confidence_level\": confidence_level\n",
    "            },\n",
    "            \"winner\": self._determine_winner(metrics_a, metrics_b, p_value, confidence_level)\n",
    "        }\n",
    "    \n",
    "    def _determine_winner(self, metrics_a: List, metrics_b: List, \n",
    "                         p_value: float, confidence_level: float) -> str:\n",
    "        \"\"\"ìŠ¹ì ê²°ì •\"\"\"\n",
    "        if p_value >= (1 - confidence_level):\n",
    "            return \"no_significant_difference\"\n",
    "        \n",
    "        mean_a = np.mean(metrics_a) if metrics_a else 0\n",
    "        mean_b = np.mean(metrics_b) if metrics_b else 0\n",
    "        \n",
    "        return \"graph_a\" if mean_a > mean_b else \"graph_b\"\n",
    "    \n",
    "    def _print_analysis(self, analysis: Dict[str, Any]):\n",
    "        \"\"\"ë¶„ì„ ê²°ê³¼ ì¶œë ¥\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\nê·¸ë˜í”„ A:\")\n",
    "        print(f\"  í‰ê·  ë©”íŠ¸ë¦­: {analysis['graph_a']['mean_metric']:.3f} (Â±{analysis['graph_a']['std_metric']:.3f})\")\n",
    "        print(f\"  í‰ê·  ì§€ì—°ì‹œê°„: {analysis['graph_a']['mean_latency']*1000:.2f}ms\")\n",
    "        print(f\"  ì„±ê³µë¥ : {analysis['graph_a']['success_rate']:.1%}\")\n",
    "        \n",
    "        print(\"\\nê·¸ë˜í”„ B:\")\n",
    "        print(f\"  í‰ê·  ë©”íŠ¸ë¦­: {analysis['graph_b']['mean_metric']:.3f} (Â±{analysis['graph_b']['std_metric']:.3f})\")\n",
    "        print(f\"  í‰ê·  ì§€ì—°ì‹œê°„: {analysis['graph_b']['mean_latency']*1000:.2f}ms\")\n",
    "        print(f\"  ì„±ê³µë¥ : {analysis['graph_b']['success_rate']:.1%}\")\n",
    "        \n",
    "        print(\"\\ní†µê³„ ê²€ì •:\")\n",
    "        print(f\"  t-í†µê³„ëŸ‰: {analysis['statistical_test']['t_statistic']:.3f}\")\n",
    "        print(f\"  p-value: {analysis['statistical_test']['p_value']:.4f}\")\n",
    "        print(f\"  í†µê³„ì  ìœ ì˜ì„±: {analysis['statistical_test']['significant']}\")\n",
    "        \n",
    "        print(f\"\\nğŸ† ìŠ¹ì: {analysis['winner']}\")\n",
    "\n",
    "# A/B í…ŒìŠ¤íŠ¸ìš© ë‘ ê°œì˜ ë‹¤ë¥¸ ê·¸ë˜í”„ ìƒì„±\n",
    "def create_graph_version_a():\n",
    "    \"\"\"ê·¸ë˜í”„ ë²„ì „ A (ê¸°ë³¸)\"\"\"\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    \n",
    "    def process(state):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    workflow.add_node(\"process\", process)\n",
    "    workflow.set_entry_point(\"process\")\n",
    "    workflow.add_edge(\"process\", END)\n",
    "    return workflow.compile()\n",
    "\n",
    "def create_graph_version_b():\n",
    "    \"\"\"ê·¸ë˜í”„ ë²„ì „ B (ê°œì„ )\"\"\"\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    \n",
    "    def process(state):\n",
    "        # ë” ì •êµí•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "        enhanced_prompt = \"Please provide a detailed and accurate response.\"\n",
    "        messages = state[\"messages\"] + [HumanMessage(content=enhanced_prompt)]\n",
    "        response = llm.invoke(messages[:-1])  # í”„ë¡¬í”„íŠ¸ëŠ” ì œì™¸\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    workflow.add_node(\"process\", process)\n",
    "    workflow.set_entry_point(\"process\")\n",
    "    workflow.add_edge(\"process\", END)\n",
    "    return workflow.compile()\n",
    "\n",
    "# A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(\"ğŸ”¬ A/B í…ŒìŠ¤íŠ¸ ë°ëª¨\\n\")\n",
    "\n",
    "graph_a = create_graph_version_a()\n",
    "graph_b = create_graph_version_b()\n",
    "\n",
    "ab_tester = ABTestFramework(graph_a, graph_b, test_dataset)\n",
    "ab_results = ab_tester.run_test(\n",
    "    metric_func=evaluate_response_length,\n",
    "    confidence_level=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Continuous Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import hashlib\n",
    "\n",
    "class ContinuousEvaluationPipeline:\n",
    "    \"\"\"ì§€ì†ì  í‰ê°€ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_name: str):\n",
    "        self.graph_name = graph_name\n",
    "        self.evaluation_history = []\n",
    "        self.baselines = {}\n",
    "        self.alerts = []\n",
    "    \n",
    "    def add_evaluation(self, \n",
    "                      graph,\n",
    "                      dataset: EvaluationDataset,\n",
    "                      version: str = None):\n",
    "        \"\"\"í‰ê°€ ì¶”ê°€\"\"\"\n",
    "        \n",
    "        # ë²„ì „ ìƒì„±\n",
    "        if version is None:\n",
    "            version = self._generate_version(graph)\n",
    "        \n",
    "        # í‰ê°€ ì‹¤í–‰\n",
    "        evaluator = GraphEvaluator(graph, dataset)\n",
    "        results = evaluator.evaluate(\n",
    "            eval_functions=[evaluate_response_length, evaluate_keyword_presence],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # ê¸°ë¡ ì €ì¥\n",
    "        evaluation_record = {\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"version\": version,\n",
    "            \"dataset_name\": dataset.name,\n",
    "            \"results\": results,\n",
    "            \"detailed_results\": evaluator.results\n",
    "        }\n",
    "        \n",
    "        self.evaluation_history.append(evaluation_record)\n",
    "        \n",
    "        # ì„±ëŠ¥ ì²´í¬\n",
    "        self._check_performance_regression(evaluation_record)\n",
    "        \n",
    "        return evaluation_record\n",
    "    \n",
    "    def _generate_version(self, graph) -> str:\n",
    "        \"\"\"ê·¸ë˜í”„ ë²„ì „ ìƒì„±\"\"\"\n",
    "        # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ë²„ì „\n",
    "        graph_str = str(graph)\n",
    "        hash_obj = hashlib.md5(graph_str.encode())\n",
    "        return hash_obj.hexdigest()[:8]\n",
    "    \n",
    "    def _check_performance_regression(self, current_eval: Dict):\n",
    "        \"\"\"ì„±ëŠ¥ íšŒê·€ ì²´í¬\"\"\"\n",
    "        if not self.baselines:\n",
    "            # ì²« í‰ê°€ë¥¼ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ì„¤ì •\n",
    "            self.baselines = current_eval[\"results\"]\n",
    "            return\n",
    "        \n",
    "        # ì£¼ìš” ë©”íŠ¸ë¦­ ë¹„êµ\n",
    "        for metric, baseline_value in self.baselines.items():\n",
    "            if metric in current_eval[\"results\"]:\n",
    "                current_value = current_eval[\"results\"][metric]\n",
    "                \n",
    "                # íšŒê·€ ê°ì§€ (10% ì´ìƒ ì„±ëŠ¥ í•˜ë½)\n",
    "                if isinstance(baseline_value, (int, float)):\n",
    "                    if current_value < baseline_value * 0.9:\n",
    "                        alert = {\n",
    "                            \"type\": \"performance_regression\",\n",
    "                            \"metric\": metric,\n",
    "                            \"baseline\": baseline_value,\n",
    "                            \"current\": current_value,\n",
    "                            \"timestamp\": datetime.now(),\n",
    "                            \"version\": current_eval[\"version\"]\n",
    "                        }\n",
    "                        self.alerts.append(alert)\n",
    "                        self._send_alert(alert)\n",
    "    \n",
    "    def _send_alert(self, alert: Dict):\n",
    "        \"\"\"ì•Œë¦¼ ì „ì†¡\"\"\"\n",
    "        print(f\"\\nâš ï¸ ì„±ëŠ¥ ì•Œë¦¼:\")\n",
    "        print(f\"  ë©”íŠ¸ë¦­: {alert['metric']}\")\n",
    "        print(f\"  ë² ì´ìŠ¤ë¼ì¸: {alert['baseline']:.3f}\")\n",
    "        print(f\"  í˜„ì¬: {alert['current']:.3f}\")\n",
    "        print(f\"  í•˜ë½ë¥ : {(1 - alert['current']/alert['baseline'])*100:.1f}%\")\n",
    "    \n",
    "    def get_trend_analysis(self, metric: str, window: int = 5) -> Dict:\n",
    "        \"\"\"íŠ¸ë Œë“œ ë¶„ì„\"\"\"\n",
    "        if len(self.evaluation_history) < 2:\n",
    "            return {\"trend\": \"insufficient_data\"}\n",
    "        \n",
    "        # ìµœê·¼ Nê°œ í‰ê°€ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ\n",
    "        recent_evals = self.evaluation_history[-window:]\n",
    "        values = []\n",
    "        timestamps = []\n",
    "        \n",
    "        for eval_record in recent_evals:\n",
    "            if metric in eval_record[\"results\"]:\n",
    "                values.append(eval_record[\"results\"][metric])\n",
    "                timestamps.append(eval_record[\"timestamp\"])\n",
    "        \n",
    "        if len(values) < 2:\n",
    "            return {\"trend\": \"insufficient_data\"}\n",
    "        \n",
    "        # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚°\n",
    "        x = np.arange(len(values))\n",
    "        slope = np.polyfit(x, values, 1)[0]\n",
    "        \n",
    "        return {\n",
    "            \"trend\": \"improving\" if slope > 0 else \"declining\",\n",
    "            \"slope\": slope,\n",
    "            \"values\": values,\n",
    "            \"timestamps\": timestamps,\n",
    "            \"mean\": np.mean(values),\n",
    "            \"std\": np.std(values)\n",
    "        }\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"í‰ê°€ ë³´ê³ ì„œ ìƒì„±\"\"\"\n",
    "        if not self.evaluation_history:\n",
    "            return \"í‰ê°€ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        latest = self.evaluation_history[-1]\n",
    "        \n",
    "        report = f\"\"\"\n",
    "        =====================================\n",
    "        í‰ê°€ ë³´ê³ ì„œ: {self.graph_name}\n",
    "        =====================================\n",
    "        \n",
    "        ìµœì‹  í‰ê°€:\n",
    "        - ë²„ì „: {latest['version']}\n",
    "        - ì‹œê°„: {latest['timestamp']}\n",
    "        - ë°ì´í„°ì…‹: {latest['dataset_name']}\n",
    "        \n",
    "        ì„±ëŠ¥ ë©”íŠ¸ë¦­:\n",
    "        - ì„±ê³µë¥ : {latest['results'].get('success_rate', 0):.1%}\n",
    "        - í‰ê·  ì§€ì—°ì‹œê°„: {latest['results'].get('avg_latency', 0)*1000:.2f}ms\n",
    "        \n",
    "        íŠ¸ë Œë“œ ë¶„ì„:\n",
    "        \"\"\"\n",
    "        \n",
    "        # ì£¼ìš” ë©”íŠ¸ë¦­ íŠ¸ë Œë“œ\n",
    "        for metric in ['success_rate', 'avg_latency']:\n",
    "            trend = self.get_trend_analysis(metric)\n",
    "            if trend[\"trend\"] != \"insufficient_data\":\n",
    "                report += f\"\\n        - {metric}: {trend['trend']} (ê¸°ìš¸ê¸°: {trend['slope']:.4f})\"\n",
    "        \n",
    "        # ì•Œë¦¼\n",
    "        if self.alerts:\n",
    "            report += f\"\\n\\n        ìµœê·¼ ì•Œë¦¼: {len(self.alerts)}ê°œ\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# ì§€ì†ì  í‰ê°€ íŒŒì´í”„ë¼ì¸ ë°ëª¨\n",
    "print(\"ğŸ”„ ì§€ì†ì  í‰ê°€ íŒŒì´í”„ë¼ì¸ ë°ëª¨\\n\")\n",
    "\n",
    "pipeline = ContinuousEvaluationPipeline(\"sample_graph\")\n",
    "\n",
    "# ì—¬ëŸ¬ ë²„ì „ í‰ê°€\n",
    "versions = [\n",
    "    (create_graph_version_a(), \"v1.0.0\"),\n",
    "    (create_graph_version_b(), \"v1.1.0\"),\n",
    "    (create_graph_version_a(), \"v1.2.0\")  # ì˜ë„ì ìœ¼ë¡œ ì„±ëŠ¥ í•˜ë½\n",
    "]\n",
    "\n",
    "for graph, version in versions:\n",
    "    print(f\"í‰ê°€ ì¤‘: {version}\")\n",
    "    eval_record = pipeline.add_evaluation(\n",
    "        graph=graph,\n",
    "        dataset=test_dataset,\n",
    "        version=version\n",
    "    )\n",
    "    time.sleep(0.5)  # ì‹œê°„ ê°„ê²© ì‹œë®¬ë ˆì´ì…˜\n",
    "\n",
    "# ë³´ê³ ì„œ ìƒì„±\n",
    "print(pipeline.generate_report())\n",
    "\n",
    "# íŠ¸ë Œë“œ ë¶„ì„\n",
    "print(\"\\nğŸ“ˆ ì„±ê³µë¥  íŠ¸ë Œë“œ:\")\n",
    "trend = pipeline.get_trend_analysis(\"success_rate\")\n",
    "if trend[\"trend\"] != \"insufficient_data\":\n",
    "    print(f\"  íŠ¸ë Œë“œ: {trend['trend']}\")\n",
    "    print(f\"  í‰ê· : {trend['mean']:.3f}\")\n",
    "    print(f\"  í‘œì¤€í¸ì°¨: {trend['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "1. ì‹¤ì œ í”„ë¡œë•ì…˜ ë°ì´í„°ë¡œ í‰ê°€ ë°ì´í„°ì…‹ êµ¬ì¶•\n",
    "2. ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­ ê°œë°œ (ë„ë©”ì¸ íŠ¹í™”)\n",
    "3. ìë™í™”ëœ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ CI/CDì— í†µí•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì‹¤ìŠµ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}