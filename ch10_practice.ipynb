{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: LangGraph Evaluation 실습\n",
    "\n",
    "이 노트북은 LangGraph 애플리케이션의 평가와 성능 측정을 실습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"OpenAI API Key를 입력하세요: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# 평가 데이터셋 클래스\n",
    "@dataclass\n",
    "class EvaluationCase:\n",
    "    \"\"\"평가 케이스\"\"\"\n",
    "    id: str\n",
    "    input: Dict[str, Any]\n",
    "    expected_output: Dict[str, Any]\n",
    "    metadata: Dict[str, Any]\n",
    "    tags: List[str]\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "\n",
    "class EvaluationDataset:\n",
    "    \"\"\"평가 데이터셋 관리\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str = \"\"):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.cases: List[EvaluationCase] = []\n",
    "        self.created_at = datetime.now()\n",
    "        self.version = \"1.0.0\"\n",
    "    \n",
    "    def add_case(self, \n",
    "                 input_data: Dict,\n",
    "                 expected_output: Dict,\n",
    "                 metadata: Dict = None,\n",
    "                 tags: List[str] = None):\n",
    "        \"\"\"평가 케이스 추가\"\"\"\n",
    "        case = EvaluationCase(\n",
    "            id=str(uuid.uuid4()),\n",
    "            input=input_data,\n",
    "            expected_output=expected_output,\n",
    "            metadata=metadata or {},\n",
    "            tags=tags or []\n",
    "        )\n",
    "        self.cases.append(case)\n",
    "    \n",
    "    def save_to_file(self, filepath: str):\n",
    "        \"\"\"파일로 저장\"\"\"\n",
    "        data = {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description,\n",
    "            \"version\": self.version,\n",
    "            \"created_at\": self.created_at.isoformat(),\n",
    "            \"cases\": [case.to_dict() for case in self.cases]\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"데이터셋이 {filepath}에 저장되었습니다.\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_file(cls, filepath: str):\n",
    "        \"\"\"파일에서 로드\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        dataset = cls(data[\"name\"], data.get(\"description\", \"\"))\n",
    "        dataset.version = data.get(\"version\", \"1.0.0\")\n",
    "        \n",
    "        for case_data in data[\"cases\"]:\n",
    "            case = EvaluationCase(**case_data)\n",
    "            dataset.cases.append(case)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"DataFrame으로 변환\"\"\"\n",
    "        return pd.DataFrame([case.to_dict() for case in self.cases])\n",
    "    \n",
    "    def filter_by_tags(self, tags: List[str]) -> List[EvaluationCase]:\n",
    "        \"\"\"태그로 필터링\"\"\"\n",
    "        filtered = []\n",
    "        for case in self.cases:\n",
    "            if any(tag in case.tags for tag in tags):\n",
    "                filtered.append(case)\n",
    "        return filtered\n",
    "\n",
    "# RAG 평가 데이터셋 생성\n",
    "def create_rag_dataset() -> EvaluationDataset:\n",
    "    \"\"\"RAG 시스템 평가 데이터셋 생성\"\"\"\n",
    "    dataset = EvaluationDataset(\n",
    "        name=\"RAG Evaluation Dataset\",\n",
    "        description=\"RAG 시스템 평가를 위한 질문-답변 데이터셋\"\n",
    "    )\n",
    "    \n",
    "    # 평가 케이스들\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"input\": {\"question\": \"LangChain이 무엇인가요?\"},\n",
    "            \"expected\": {\n",
    "                \"relevance\": \"high\",\n",
    "                \"keywords\": [\"프레임워크\", \"LLM\", \"애플리케이션\"]\n",
    "            },\n",
    "            \"tags\": [\"definition\", \"basic\"]\n",
    "        },\n",
    "        {\n",
    "            \"input\": {\"question\": \"벡터 데이터베이스의 장점은?\"},\n",
    "            \"expected\": {\n",
    "                \"relevance\": \"high\",\n",
    "                \"keywords\": [\"유사도\", \"검색\", \"임베딩\"]\n",
    "            },\n",
    "            \"tags\": [\"technical\", \"database\"]\n",
    "        },\n",
    "        {\n",
    "            \"input\": {\"question\": \"RAG와 파인튜닝의 차이점은?\"},\n",
    "            \"expected\": {\n",
    "                \"relevance\": \"high\",\n",
    "                \"keywords\": [\"검색\", \"학습\", \"비용\", \"유연성\"]\n",
    "            },\n",
    "            \"tags\": [\"comparison\", \"advanced\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        dataset.add_case(\n",
    "            input_data=case[\"input\"],\n",
    "            expected_output=case[\"expected\"],\n",
    "            tags=case[\"tags\"]\n",
    "        )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# SQL 평가 데이터셋 생성\n",
    "def create_sql_dataset() -> EvaluationDataset:\n",
    "    \"\"\"SQL 생성 평가 데이터셋\"\"\"\n",
    "    dataset = EvaluationDataset(\n",
    "        name=\"SQL Generation Dataset\",\n",
    "        description=\"SQL 쿼리 생성 평가 데이터셋\"\n",
    "    )\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"input\": {\"question\": \"모든 직원의 이름과 부서를 조회하세요\"},\n",
    "            \"expected\": {\n",
    "                \"query_type\": \"SELECT\",\n",
    "                \"tables\": [\"employees\"],\n",
    "                \"columns\": [\"name\", \"department\"]\n",
    "            },\n",
    "            \"tags\": [\"basic\", \"select\"]\n",
    "        },\n",
    "        {\n",
    "            \"input\": {\"question\": \"부서별 평균 급여를 계산하세요\"},\n",
    "            \"expected\": {\n",
    "                \"query_type\": \"SELECT\",\n",
    "                \"functions\": [\"AVG\"],\n",
    "                \"group_by\": True\n",
    "            },\n",
    "            \"tags\": [\"aggregation\", \"group_by\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        dataset.add_case(\n",
    "            input_data=case[\"input\"],\n",
    "            expected_output=case[\"expected\"],\n",
    "            tags=case[\"tags\"]\n",
    "        )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# 데이터셋 생성 및 저장\n",
    "rag_dataset = create_rag_dataset()\n",
    "sql_dataset = create_sql_dataset()\n",
    "\n",
    "print(\"📊 RAG 데이터셋:\")\n",
    "print(f\"케이스 수: {len(rag_dataset.cases)}\")\n",
    "print(rag_dataset.to_dataframe()[[\"id\", \"tags\"]].head())\n",
    "\n",
    "print(\"\\n📊 SQL 데이터셋:\")\n",
    "print(f\"케이스 수: {len(sql_dataset.cases)}\")\n",
    "print(sql_dataset.to_dataframe()[[\"id\", \"tags\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 평가 메트릭 클래스\n",
    "class EvaluationMetrics:\n",
    "    \"\"\"평가 메트릭 계산\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_accuracy(predictions: List, ground_truth: List) -> float:\n",
    "        \"\"\"정확도 계산\"\"\"\n",
    "        if len(predictions) != len(ground_truth):\n",
    "            raise ValueError(\"예측과 정답의 길이가 다릅니다.\")\n",
    "        \n",
    "        correct = sum(1 for p, g in zip(predictions, ground_truth) if p == g)\n",
    "        return correct / len(predictions)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_bleu_score(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"BLEU 점수 계산 (간단한 버전)\"\"\"\n",
    "        from nltk.translate.bleu_score import sentence_bleu\n",
    "        reference_tokens = reference.split()\n",
    "        hypothesis_tokens = hypothesis.split()\n",
    "        return sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rouge_score(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "        \"\"\"ROUGE 점수 계산\"\"\"\n",
    "        # 간단한 ROUGE-1 구현\n",
    "        ref_tokens = set(reference.lower().split())\n",
    "        hyp_tokens = set(hypothesis.lower().split())\n",
    "        \n",
    "        if not hyp_tokens:\n",
    "            return {\"precision\": 0, \"recall\": 0, \"f1\": 0}\n",
    "        \n",
    "        overlap = ref_tokens.intersection(hyp_tokens)\n",
    "        precision = len(overlap) / len(hyp_tokens) if hyp_tokens else 0\n",
    "        recall = len(overlap) / len(ref_tokens) if ref_tokens else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_semantic_similarity(text1: str, text2: str, embeddings) -> float:\n",
    "        \"\"\"의미적 유사도 계산\"\"\"\n",
    "        emb1 = embeddings.embed_query(text1)\n",
    "        emb2 = embeddings.embed_query(text2)\n",
    "        \n",
    "        # 코사인 유사도\n",
    "        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "        return float(similarity)\n",
    "\n",
    "# LLM 기반 평가자\n",
    "class LLMEvaluator:\n",
    "    \"\"\"LLM을 사용한 평가\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=0)\n",
    "    \n",
    "    def evaluate_relevance(self, question: str, answer: str) -> Dict[str, Any]:\n",
    "        \"\"\"관련성 평가\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        질문: {question}\n",
    "        답변: {answer}\n",
    "        \n",
    "        위 답변이 질문에 얼마나 관련이 있는지 평가하세요.\n",
    "        점수: 1-10 (10이 가장 관련성 높음)\n",
    "        이유를 간단히 설명하세요.\n",
    "        \n",
    "        JSON 형식으로 응답: {{\"score\": <점수>, \"reason\": \"<이유>\"}}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.content)\n",
    "            return result\n",
    "        except:\n",
    "            return {\"score\": 0, \"reason\": \"평가 실패\"}\n",
    "    \n",
    "    def evaluate_correctness(self, answer: str, expected: str) -> Dict[str, Any]:\n",
    "        \"\"\"정확성 평가\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        생성된 답변: {answer}\n",
    "        예상 답변: {expected}\n",
    "        \n",
    "        생성된 답변이 예상 답변과 얼마나 일치하는지 평가하세요.\n",
    "        의미적으로 같은 내용이면 높은 점수를 주세요.\n",
    "        \n",
    "        점수: 1-10\n",
    "        JSON 형식: {{\"score\": <점수>, \"differences\": [\"<차이점들>\"]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.content)\n",
    "            return result\n",
    "        except:\n",
    "            return {\"score\": 0, \"differences\": [\"평가 실패\"]}\n",
    "    \n",
    "    def evaluate_hallucination(self, answer: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"환각 평가\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        컨텍스트: {context}\n",
    "        생성된 답변: {answer}\n",
    "        \n",
    "        답변에 컨텍스트에 없는 정보(환각)가 포함되어 있는지 확인하세요.\n",
    "        \n",
    "        JSON 형식: {{\n",
    "            \"has_hallucination\": <true/false>,\n",
    "            \"hallucinated_parts\": [\"<환각 부분들>\"],\n",
    "            \"confidence\": <0-1>\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.content)\n",
    "            return result\n",
    "        except:\n",
    "            return {\n",
    "                \"has_hallucination\": False,\n",
    "                \"hallucinated_parts\": [],\n",
    "                \"confidence\": 0\n",
    "            }\n",
    "\n",
    "# 메트릭 테스트\n",
    "metrics = EvaluationMetrics()\n",
    "llm_evaluator = LLMEvaluator()\n",
    "\n",
    "# 예제 텍스트\n",
    "reference = \"LangChain은 대규모 언어 모델을 활용한 애플리케이션 개발 프레임워크입니다.\"\n",
    "hypothesis = \"LangChain은 LLM 기반 앱 개발을 위한 프레임워크입니다.\"\n",
    "\n",
    "# ROUGE 점수\n",
    "rouge_scores = metrics.calculate_rouge_score(reference, hypothesis)\n",
    "print(\"📊 ROUGE 점수:\")\n",
    "print(f\"Precision: {rouge_scores['precision']:.3f}\")\n",
    "print(f\"Recall: {rouge_scores['recall']:.3f}\")\n",
    "print(f\"F1: {rouge_scores['f1']:.3f}\")\n",
    "\n",
    "# LLM 평가\n",
    "print(\"\\n🤖 LLM 기반 평가:\")\n",
    "relevance = llm_evaluator.evaluate_relevance(\n",
    "    \"LangChain이 무엇인가요?\",\n",
    "    hypothesis\n",
    ")\n",
    "print(f\"관련성 점수: {relevance.get('score', 0)}/10\")\n",
    "print(f\"이유: {relevance.get('reason', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langchain_core.messages import HumanMessage\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "# 그래프 평가 프레임워크\n",
    "class GraphEvaluator:\n",
    "    \"\"\"LangGraph 평가 프레임워크\"\"\"\n",
    "    \n",
    "    def __init__(self, graph, dataset: EvaluationDataset):\n",
    "        self.graph = graph\n",
    "        self.dataset = dataset\n",
    "        self.results = []\n",
    "        self.metrics = EvaluationMetrics()\n",
    "        self.llm_evaluator = LLMEvaluator()\n",
    "    \n",
    "    def evaluate(self, \n",
    "                 eval_functions: List[Callable] = None,\n",
    "                 verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"전체 평가 실행\"\"\"\n",
    "        \n",
    "        total_cases = len(self.dataset.cases)\n",
    "        print(f\"📊 평가 시작: {total_cases}개 케이스\")\n",
    "        \n",
    "        for i, case in enumerate(self.dataset.cases, 1):\n",
    "            if verbose:\n",
    "                print(f\"\\n평가 중 [{i}/{total_cases}]: {case.id}\")\n",
    "            \n",
    "            # 그래프 실행\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                output = self.graph.invoke(case.input)\n",
    "                latency = time.time() - start_time\n",
    "                success = True\n",
    "                error = None\n",
    "            except Exception as e:\n",
    "                output = None\n",
    "                latency = time.time() - start_time\n",
    "                success = False\n",
    "                error = str(e)\n",
    "            \n",
    "            # 평가 실행\n",
    "            eval_results = {\n",
    "                \"case_id\": case.id,\n",
    "                \"success\": success,\n",
    "                \"latency\": latency,\n",
    "                \"error\": error\n",
    "            }\n",
    "            \n",
    "            if success and eval_functions:\n",
    "                for eval_func in eval_functions:\n",
    "                    metric_name = eval_func.__name__\n",
    "                    metric_value = eval_func(output, case.expected_output)\n",
    "                    eval_results[metric_name] = metric_value\n",
    "            \n",
    "            self.results.append(eval_results)\n",
    "        \n",
    "        # 집계\n",
    "        summary = self._calculate_summary()\n",
    "        \n",
    "        if verbose:\n",
    "            self._print_summary(summary)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _calculate_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"평가 결과 요약\"\"\"\n",
    "        successful_results = [r for r in self.results if r[\"success\"]]\n",
    "        \n",
    "        summary = {\n",
    "            \"total_cases\": len(self.results),\n",
    "            \"successful_cases\": len(successful_results),\n",
    "            \"failed_cases\": len(self.results) - len(successful_results),\n",
    "            \"success_rate\": len(successful_results) / len(self.results) if self.results else 0,\n",
    "            \"avg_latency\": np.mean([r[\"latency\"] for r in self.results]) if self.results else 0,\n",
    "            \"p95_latency\": np.percentile([r[\"latency\"] for r in self.results], 95) if self.results else 0\n",
    "        }\n",
    "        \n",
    "        # 커스텀 메트릭 집계\n",
    "        metric_names = set()\n",
    "        for result in successful_results:\n",
    "            metric_names.update(k for k in result.keys() \n",
    "                              if k not in [\"case_id\", \"success\", \"latency\", \"error\"])\n",
    "        \n",
    "        for metric in metric_names:\n",
    "            values = [r.get(metric, 0) for r in successful_results if metric in r]\n",
    "            if values:\n",
    "                summary[f\"avg_{metric}\"] = np.mean(values)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _print_summary(self, summary: Dict[str, Any]):\n",
    "        \"\"\"요약 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📈 평가 결과 요약\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"총 케이스: {summary['total_cases']}\")\n",
    "        print(f\"성공: {summary['successful_cases']} ({summary['success_rate']:.1%})\")\n",
    "        print(f\"실패: {summary['failed_cases']}\")\n",
    "        print(f\"\\n평균 지연시간: {summary['avg_latency']*1000:.2f}ms\")\n",
    "        print(f\"P95 지연시간: {summary['p95_latency']*1000:.2f}ms\")\n",
    "        \n",
    "        # 커스텀 메트릭\n",
    "        custom_metrics = {k: v for k, v in summary.items() \n",
    "                         if k.startswith(\"avg_\") and not k.endswith(\"latency\")}\n",
    "        if custom_metrics:\n",
    "            print(\"\\n커스텀 메트릭:\")\n",
    "            for metric, value in custom_metrics.items():\n",
    "                print(f\"  {metric}: {value:.3f}\")\n",
    "    \n",
    "    def export_results(self, filepath: str):\n",
    "        \"\"\"결과 내보내기\"\"\"\n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"결과가 {filepath}에 저장되었습니다.\")\n",
    "\n",
    "# 샘플 그래프 생성\n",
    "def create_sample_graph():\n",
    "    \"\"\"평가용 샘플 그래프\"\"\"\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    \n",
    "    def process_node(state: MessagesState):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    workflow.add_node(\"process\", process_node)\n",
    "    workflow.set_entry_point(\"process\")\n",
    "    workflow.add_edge(\"process\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# 평가 함수 정의\n",
    "def evaluate_response_length(output: Dict, expected: Dict) -> float:\n",
    "    \"\"\"응답 길이 평가\"\"\"\n",
    "    if \"messages\" in output and output[\"messages\"]:\n",
    "        response_length = len(output[\"messages\"][-1].content)\n",
    "        return min(response_length / 100, 1.0)  # 100자를 기준으로 정규화\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_keyword_presence(output: Dict, expected: Dict) -> float:\n",
    "    \"\"\"키워드 포함 평가\"\"\"\n",
    "    if \"keywords\" not in expected:\n",
    "        return 1.0\n",
    "    \n",
    "    if \"messages\" in output and output[\"messages\"]:\n",
    "        response = output[\"messages\"][-1].content.lower()\n",
    "        keywords = expected.get(\"keywords\", [])\n",
    "        \n",
    "        if keywords:\n",
    "            present = sum(1 for kw in keywords if kw.lower() in response)\n",
    "            return present / len(keywords)\n",
    "    return 0.0\n",
    "\n",
    "# 평가 실행\n",
    "print(\"🚀 그래프 평가 시작\\n\")\n",
    "\n",
    "# 테스트용 간단한 데이터셋\n",
    "test_dataset = EvaluationDataset(\"Test Dataset\")\n",
    "test_dataset.add_case(\n",
    "    input_data={\"messages\": [HumanMessage(content=\"LangChain이 뭐야?\")]},\n",
    "    expected_output={\"keywords\": [\"프레임워크\", \"LLM\"]},\n",
    "    tags=[\"basic\"]\n",
    ")\n",
    "test_dataset.add_case(\n",
    "    input_data={\"messages\": [HumanMessage(content=\"Python 설명해줘\")]},\n",
    "    expected_output={\"keywords\": [\"프로그래밍\", \"언어\"]},\n",
    "    tags=[\"basic\"]\n",
    ")\n",
    "\n",
    "# 그래프 생성 및 평가\n",
    "sample_graph = create_sample_graph()\n",
    "evaluator = GraphEvaluator(sample_graph, test_dataset)\n",
    "\n",
    "# 평가 실행\n",
    "summary = evaluator.evaluate(\n",
    "    eval_functions=[evaluate_response_length, evaluate_keyword_presence],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A/B Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "class ABTestFramework:\n",
    "    \"\"\"A/B 테스팅 프레임워크\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_a, graph_b, dataset: EvaluationDataset):\n",
    "        self.graph_a = graph_a\n",
    "        self.graph_b = graph_b\n",
    "        self.dataset = dataset\n",
    "        self.results_a = []\n",
    "        self.results_b = []\n",
    "    \n",
    "    def run_test(self, \n",
    "                 metric_func: Callable,\n",
    "                 sample_size: int = None,\n",
    "                 confidence_level: float = 0.95) -> Dict[str, Any]:\n",
    "        \"\"\"A/B 테스트 실행\"\"\"\n",
    "        \n",
    "        # 샘플 크기 결정\n",
    "        cases = self.dataset.cases[:sample_size] if sample_size else self.dataset.cases\n",
    "        \n",
    "        print(f\"🔬 A/B 테스트 시작 (n={len(cases)})\")\n",
    "        \n",
    "        for case in cases:\n",
    "            # 그래프 A 평가\n",
    "            try:\n",
    "                start = time.time()\n",
    "                output_a = self.graph_a.invoke(case.input)\n",
    "                latency_a = time.time() - start\n",
    "                metric_a = metric_func(output_a, case.expected_output)\n",
    "                self.results_a.append({\n",
    "                    \"metric\": metric_a,\n",
    "                    \"latency\": latency_a,\n",
    "                    \"success\": True\n",
    "                })\n",
    "            except Exception as e:\n",
    "                self.results_a.append({\n",
    "                    \"metric\": 0,\n",
    "                    \"latency\": 0,\n",
    "                    \"success\": False\n",
    "                })\n",
    "            \n",
    "            # 그래프 B 평가\n",
    "            try:\n",
    "                start = time.time()\n",
    "                output_b = self.graph_b.invoke(case.input)\n",
    "                latency_b = time.time() - start\n",
    "                metric_b = metric_func(output_b, case.expected_output)\n",
    "                self.results_b.append({\n",
    "                    \"metric\": metric_b,\n",
    "                    \"latency\": latency_b,\n",
    "                    \"success\": True\n",
    "                })\n",
    "            except Exception as e:\n",
    "                self.results_b.append({\n",
    "                    \"metric\": 0,\n",
    "                    \"latency\": 0,\n",
    "                    \"success\": False\n",
    "                })\n",
    "        \n",
    "        # 통계 분석\n",
    "        analysis = self._analyze_results(confidence_level)\n",
    "        self._print_analysis(analysis)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_results(self, confidence_level: float) -> Dict[str, Any]:\n",
    "        \"\"\"결과 분석\"\"\"\n",
    "        metrics_a = [r[\"metric\"] for r in self.results_a if r[\"success\"]]\n",
    "        metrics_b = [r[\"metric\"] for r in self.results_b if r[\"success\"]]\n",
    "        \n",
    "        latencies_a = [r[\"latency\"] for r in self.results_a if r[\"success\"]]\n",
    "        latencies_b = [r[\"latency\"] for r in self.results_b if r[\"success\"]]\n",
    "        \n",
    "        # t-test\n",
    "        if len(metrics_a) > 1 and len(metrics_b) > 1:\n",
    "            t_stat, p_value = stats.ttest_ind(metrics_a, metrics_b)\n",
    "        else:\n",
    "            t_stat, p_value = 0, 1\n",
    "        \n",
    "        return {\n",
    "            \"graph_a\": {\n",
    "                \"mean_metric\": np.mean(metrics_a) if metrics_a else 0,\n",
    "                \"std_metric\": np.std(metrics_a) if metrics_a else 0,\n",
    "                \"mean_latency\": np.mean(latencies_a) if latencies_a else 0,\n",
    "                \"success_rate\": sum(r[\"success\"] for r in self.results_a) / len(self.results_a)\n",
    "            },\n",
    "            \"graph_b\": {\n",
    "                \"mean_metric\": np.mean(metrics_b) if metrics_b else 0,\n",
    "                \"std_metric\": np.std(metrics_b) if metrics_b else 0,\n",
    "                \"mean_latency\": np.mean(latencies_b) if latencies_b else 0,\n",
    "                \"success_rate\": sum(r[\"success\"] for r in self.results_b) / len(self.results_b)\n",
    "            },\n",
    "            \"statistical_test\": {\n",
    "                \"t_statistic\": t_stat,\n",
    "                \"p_value\": p_value,\n",
    "                \"significant\": p_value < (1 - confidence_level),\n",
    "                \"confidence_level\": confidence_level\n",
    "            },\n",
    "            \"winner\": self._determine_winner(metrics_a, metrics_b, p_value, confidence_level)\n",
    "        }\n",
    "    \n",
    "    def _determine_winner(self, metrics_a: List, metrics_b: List, \n",
    "                         p_value: float, confidence_level: float) -> str:\n",
    "        \"\"\"승자 결정\"\"\"\n",
    "        if p_value >= (1 - confidence_level):\n",
    "            return \"no_significant_difference\"\n",
    "        \n",
    "        mean_a = np.mean(metrics_a) if metrics_a else 0\n",
    "        mean_b = np.mean(metrics_b) if metrics_b else 0\n",
    "        \n",
    "        return \"graph_a\" if mean_a > mean_b else \"graph_b\"\n",
    "    \n",
    "    def _print_analysis(self, analysis: Dict[str, Any]):\n",
    "        \"\"\"분석 결과 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 A/B 테스트 결과\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\n그래프 A:\")\n",
    "        print(f\"  평균 메트릭: {analysis['graph_a']['mean_metric']:.3f} (±{analysis['graph_a']['std_metric']:.3f})\")\n",
    "        print(f\"  평균 지연시간: {analysis['graph_a']['mean_latency']*1000:.2f}ms\")\n",
    "        print(f\"  성공률: {analysis['graph_a']['success_rate']:.1%}\")\n",
    "        \n",
    "        print(\"\\n그래프 B:\")\n",
    "        print(f\"  평균 메트릭: {analysis['graph_b']['mean_metric']:.3f} (±{analysis['graph_b']['std_metric']:.3f})\")\n",
    "        print(f\"  평균 지연시간: {analysis['graph_b']['mean_latency']*1000:.2f}ms\")\n",
    "        print(f\"  성공률: {analysis['graph_b']['success_rate']:.1%}\")\n",
    "        \n",
    "        print(\"\\n통계 검정:\")\n",
    "        print(f\"  t-통계량: {analysis['statistical_test']['t_statistic']:.3f}\")\n",
    "        print(f\"  p-value: {analysis['statistical_test']['p_value']:.4f}\")\n",
    "        print(f\"  통계적 유의성: {analysis['statistical_test']['significant']}\")\n",
    "        \n",
    "        print(f\"\\n🏆 승자: {analysis['winner']}\")\n",
    "\n",
    "# A/B 테스트용 두 개의 다른 그래프 생성\n",
    "def create_graph_version_a():\n",
    "    \"\"\"그래프 버전 A (기본)\"\"\"\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    \n",
    "    def process(state):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    workflow.add_node(\"process\", process)\n",
    "    workflow.set_entry_point(\"process\")\n",
    "    workflow.add_edge(\"process\", END)\n",
    "    return workflow.compile()\n",
    "\n",
    "def create_graph_version_b():\n",
    "    \"\"\"그래프 버전 B (개선)\"\"\"\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    \n",
    "    def process(state):\n",
    "        # 더 정교한 프롬프트 사용\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "        enhanced_prompt = \"Please provide a detailed and accurate response.\"\n",
    "        messages = state[\"messages\"] + [HumanMessage(content=enhanced_prompt)]\n",
    "        response = llm.invoke(messages[:-1])  # 프롬프트는 제외\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    workflow.add_node(\"process\", process)\n",
    "    workflow.set_entry_point(\"process\")\n",
    "    workflow.add_edge(\"process\", END)\n",
    "    return workflow.compile()\n",
    "\n",
    "# A/B 테스트 실행\n",
    "print(\"🔬 A/B 테스트 데모\\n\")\n",
    "\n",
    "graph_a = create_graph_version_a()\n",
    "graph_b = create_graph_version_b()\n",
    "\n",
    "ab_tester = ABTestFramework(graph_a, graph_b, test_dataset)\n",
    "ab_results = ab_tester.run_test(\n",
    "    metric_func=evaluate_response_length,\n",
    "    confidence_level=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Continuous Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import hashlib\n",
    "\n",
    "class ContinuousEvaluationPipeline:\n",
    "    \"\"\"지속적 평가 파이프라인\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_name: str):\n",
    "        self.graph_name = graph_name\n",
    "        self.evaluation_history = []\n",
    "        self.baselines = {}\n",
    "        self.alerts = []\n",
    "    \n",
    "    def add_evaluation(self, \n",
    "                      graph,\n",
    "                      dataset: EvaluationDataset,\n",
    "                      version: str = None):\n",
    "        \"\"\"평가 추가\"\"\"\n",
    "        \n",
    "        # 버전 생성\n",
    "        if version is None:\n",
    "            version = self._generate_version(graph)\n",
    "        \n",
    "        # 평가 실행\n",
    "        evaluator = GraphEvaluator(graph, dataset)\n",
    "        results = evaluator.evaluate(\n",
    "            eval_functions=[evaluate_response_length, evaluate_keyword_presence],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # 기록 저장\n",
    "        evaluation_record = {\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"version\": version,\n",
    "            \"dataset_name\": dataset.name,\n",
    "            \"results\": results,\n",
    "            \"detailed_results\": evaluator.results\n",
    "        }\n",
    "        \n",
    "        self.evaluation_history.append(evaluation_record)\n",
    "        \n",
    "        # 성능 체크\n",
    "        self._check_performance_regression(evaluation_record)\n",
    "        \n",
    "        return evaluation_record\n",
    "    \n",
    "    def _generate_version(self, graph) -> str:\n",
    "        \"\"\"그래프 버전 생성\"\"\"\n",
    "        # 간단한 해시 기반 버전\n",
    "        graph_str = str(graph)\n",
    "        hash_obj = hashlib.md5(graph_str.encode())\n",
    "        return hash_obj.hexdigest()[:8]\n",
    "    \n",
    "    def _check_performance_regression(self, current_eval: Dict):\n",
    "        \"\"\"성능 회귀 체크\"\"\"\n",
    "        if not self.baselines:\n",
    "            # 첫 평가를 베이스라인으로 설정\n",
    "            self.baselines = current_eval[\"results\"]\n",
    "            return\n",
    "        \n",
    "        # 주요 메트릭 비교\n",
    "        for metric, baseline_value in self.baselines.items():\n",
    "            if metric in current_eval[\"results\"]:\n",
    "                current_value = current_eval[\"results\"][metric]\n",
    "                \n",
    "                # 회귀 감지 (10% 이상 성능 하락)\n",
    "                if isinstance(baseline_value, (int, float)):\n",
    "                    if current_value < baseline_value * 0.9:\n",
    "                        alert = {\n",
    "                            \"type\": \"performance_regression\",\n",
    "                            \"metric\": metric,\n",
    "                            \"baseline\": baseline_value,\n",
    "                            \"current\": current_value,\n",
    "                            \"timestamp\": datetime.now(),\n",
    "                            \"version\": current_eval[\"version\"]\n",
    "                        }\n",
    "                        self.alerts.append(alert)\n",
    "                        self._send_alert(alert)\n",
    "    \n",
    "    def _send_alert(self, alert: Dict):\n",
    "        \"\"\"알림 전송\"\"\"\n",
    "        print(f\"\\n⚠️ 성능 알림:\")\n",
    "        print(f\"  메트릭: {alert['metric']}\")\n",
    "        print(f\"  베이스라인: {alert['baseline']:.3f}\")\n",
    "        print(f\"  현재: {alert['current']:.3f}\")\n",
    "        print(f\"  하락률: {(1 - alert['current']/alert['baseline'])*100:.1f}%\")\n",
    "    \n",
    "    def get_trend_analysis(self, metric: str, window: int = 5) -> Dict:\n",
    "        \"\"\"트렌드 분석\"\"\"\n",
    "        if len(self.evaluation_history) < 2:\n",
    "            return {\"trend\": \"insufficient_data\"}\n",
    "        \n",
    "        # 최근 N개 평가에서 메트릭 추출\n",
    "        recent_evals = self.evaluation_history[-window:]\n",
    "        values = []\n",
    "        timestamps = []\n",
    "        \n",
    "        for eval_record in recent_evals:\n",
    "            if metric in eval_record[\"results\"]:\n",
    "                values.append(eval_record[\"results\"][metric])\n",
    "                timestamps.append(eval_record[\"timestamp\"])\n",
    "        \n",
    "        if len(values) < 2:\n",
    "            return {\"trend\": \"insufficient_data\"}\n",
    "        \n",
    "        # 선형 회귀로 트렌드 계산\n",
    "        x = np.arange(len(values))\n",
    "        slope = np.polyfit(x, values, 1)[0]\n",
    "        \n",
    "        return {\n",
    "            \"trend\": \"improving\" if slope > 0 else \"declining\",\n",
    "            \"slope\": slope,\n",
    "            \"values\": values,\n",
    "            \"timestamps\": timestamps,\n",
    "            \"mean\": np.mean(values),\n",
    "            \"std\": np.std(values)\n",
    "        }\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"평가 보고서 생성\"\"\"\n",
    "        if not self.evaluation_history:\n",
    "            return \"평가 기록이 없습니다.\"\n",
    "        \n",
    "        latest = self.evaluation_history[-1]\n",
    "        \n",
    "        report = f\"\"\"\n",
    "        =====================================\n",
    "        평가 보고서: {self.graph_name}\n",
    "        =====================================\n",
    "        \n",
    "        최신 평가:\n",
    "        - 버전: {latest['version']}\n",
    "        - 시간: {latest['timestamp']}\n",
    "        - 데이터셋: {latest['dataset_name']}\n",
    "        \n",
    "        성능 메트릭:\n",
    "        - 성공률: {latest['results'].get('success_rate', 0):.1%}\n",
    "        - 평균 지연시간: {latest['results'].get('avg_latency', 0)*1000:.2f}ms\n",
    "        \n",
    "        트렌드 분석:\n",
    "        \"\"\"\n",
    "        \n",
    "        # 주요 메트릭 트렌드\n",
    "        for metric in ['success_rate', 'avg_latency']:\n",
    "            trend = self.get_trend_analysis(metric)\n",
    "            if trend[\"trend\"] != \"insufficient_data\":\n",
    "                report += f\"\\n        - {metric}: {trend['trend']} (기울기: {trend['slope']:.4f})\"\n",
    "        \n",
    "        # 알림\n",
    "        if self.alerts:\n",
    "            report += f\"\\n\\n        최근 알림: {len(self.alerts)}개\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# 지속적 평가 파이프라인 데모\n",
    "print(\"🔄 지속적 평가 파이프라인 데모\\n\")\n",
    "\n",
    "pipeline = ContinuousEvaluationPipeline(\"sample_graph\")\n",
    "\n",
    "# 여러 버전 평가\n",
    "versions = [\n",
    "    (create_graph_version_a(), \"v1.0.0\"),\n",
    "    (create_graph_version_b(), \"v1.1.0\"),\n",
    "    (create_graph_version_a(), \"v1.2.0\")  # 의도적으로 성능 하락\n",
    "]\n",
    "\n",
    "for graph, version in versions:\n",
    "    print(f\"평가 중: {version}\")\n",
    "    eval_record = pipeline.add_evaluation(\n",
    "        graph=graph,\n",
    "        dataset=test_dataset,\n",
    "        version=version\n",
    "    )\n",
    "    time.sleep(0.5)  # 시간 간격 시뮬레이션\n",
    "\n",
    "# 보고서 생성\n",
    "print(pipeline.generate_report())\n",
    "\n",
    "# 트렌드 분석\n",
    "print(\"\\n📈 성공률 트렌드:\")\n",
    "trend = pipeline.get_trend_analysis(\"success_rate\")\n",
    "if trend[\"trend\"] != \"insufficient_data\":\n",
    "    print(f\"  트렌드: {trend['trend']}\")\n",
    "    print(f\"  평균: {trend['mean']:.3f}\")\n",
    "    print(f\"  표준편차: {trend['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 과제\n",
    "\n",
    "1. 실제 프로덕션 데이터로 평가 데이터셋 구축\n",
    "2. 커스텀 평가 메트릭 개발 (도메인 특화)\n",
    "3. 자동화된 평가 파이프라인을 CI/CD에 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하세요\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}